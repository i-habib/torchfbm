{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"torchfbm: Fractional Brownian Motion in PyTorch","text":"<p>torchfbm is a PyTorch library for the efficient generation, analysis, and modeling of Fractional Brownian Motion (fBm) and Fractional Gaussian Noise (fGn). It provides GPU-accelerated solvers, differentiable estimators, and neural network layers designed for stochastic modeling and anomalous diffusion research.</p> <p>The library bridges the gap between classical stochastic calculus and modern deep learning, enabling end-to-end training of systems governed by fractional noise.</p>"},{"location":"#key-capabilities","title":"Key Capabilities","text":"<ul> <li>Efficient Generation: Implements the Davies-Harte algorithm (Exact FFT-based simulation) with \\(O(N \\log N)\\) complexity, fully vectorized for batched GPU generation, as well as Cholesky Decomposition for exact covariance matrix factorization.</li> <li>Differentiable Estimators: Includes Detrended Fluctuation Analysis (DFA) and spectral estimators that allow gradients to propagate through the Hurst exponent estimation.</li> <li>Neural SDE Solvers: Provides Euler-Maruyama solvers for Fractional Stochastic Differential Equations (fSDEs) with learnable drift, diffusion, and Hurst parameters.</li> <li>Deep Learning Integration: Features <code>FBMNoisyLinear</code> layers for structured exploration in Reinforcement Learning and <code>FractionalKernel</code> for Gaussian Processes.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install torchfbm\n</code></pre>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#generating-correlated-noise","title":"Generating Correlated Noise","text":"<p><code>torchfbm</code> serves as a drop-in replacement for standard Gaussian noise when temporal correlation is required.</p> <pre><code>import torch\nfrom torchfbm import generate_davies_harte\n\n# Generate a batch of 100 paths, each with 10,000 steps\n# H=0.7 implies persistent memory (long-range dependence)\nnoise = generate_davies_harte(\n    n=10000, \n    H=0.7, \n    size=(100,), \n    device='cuda'\n)\n\nprint(f\"Shape: {noise.shape}\")  # torch.Size([100, 10000])\nprint(f\"Device: {noise.device}\") # cuda:0\n</code></pre>"},{"location":"#solving-a-fractional-sde","title":"Solving a Fractional SDE","text":"<p>Solve equations of the form \\(dX_t = \\mu(X_t)dt + \\sigma(X_t)dB^H_t\\).</p> <pre><code>from torch import nn\nfrom torchfbm.sde import NeuralFSDE\n\n# Define drift and diffusion networks\ndrift_net = nn.Linear(1, 1)\ndiffusion_net = nn.Linear(1, 1)\n\n# Initialize solver with learnable Hurst parameter\nfsde = NeuralFSDE(\n    state_size=1, \n    drift_net=drift_net, \n    diffusion_net=diffusion_net,\n    learnable_H=True,\n    H_init=0.7\n)\n\n# Integrate trajectory\nx0 = torch.zeros(32, 1).cuda()\ntrajectory = fsde(x0, n_steps=1000)\n</code></pre>"},{"location":"#mathematical-foundation","title":"Mathematical Foundation","text":"<p>Fractional Brownian Motion \\(B_H(t)\\) is a continuous-time Gaussian process characterized by the Hurst exponent \\(H \\in (0, 1)\\). Its covariance function is given by Mandelbrot &amp; Van Ness (1968):</p> \\[ E[B_H(t)B_H(s)] = \\frac{1}{2} \\left(|t|^{2H} + |s|^{2H} - |t-s|^{2H} \\right) \\] <ul> <li>\\(H &lt; 0.5\\): Anti-persistent (rough volatility, mean-reverting).</li> <li>\\(H = 0.5\\): Standard Brownian Motion (uncorrelated increments).</li> <li>\\(H &gt; 0.5\\): Persistent (trending, long memory).</li> </ul>"},{"location":"#citation","title":"Citation","text":"<p>If you use <code>torchfbm</code> in your research, please cite:</p> <pre><code>@software{torchfbm2026,\n  author = {Ivan Habib},\n  title = {TorchFBM: High-performance Fractional Brownian Motion toolkit for PyTorch},\n  year = {2026},\n  url = {https://github.com/i-habib/torchfbm}\n}\n</code></pre>"},{"location":"api/","title":"API Reference","text":"<p>This page documents the complete API of the <code>torchfbm</code> library.</p>"},{"location":"api/#generators","title":"Generators","text":"<p>Core functions for generating fractional Brownian motion (fBm) and  fractional Gaussian noise (fGn).</p>"},{"location":"api/#torchfbm.generators","title":"<code>torchfbm.generators</code>","text":"<p>Fractional Brownian Motion and Fractional Gaussian Noise generators.</p> <p>This module provides efficient algorithms for generating fBm paths and fGn samples, including both exact (Cholesky) and approximate (Davies-Harte) methods.</p> <p>The core mathematical foundation is based on Mandelbrot &amp; Van Ness (1968), with generation algorithms from Davies &amp; Harte (1987) and Asmussen &amp; Glynn (2007).</p> Example <p>from torchfbm import fbm, generate_davies_harte</p>"},{"location":"api/#torchfbm.generators--generate-fbm-path-with-h07","title":"Generate fBm path with H=0.7","text":"<p>path = fbm(n=1000, H=0.7, size=(10,))</p>"},{"location":"api/#torchfbm.generators--generate-fgn-increments","title":"Generate fGn increments","text":"<p>noise = generate_davies_harte(n=1000, H=0.7, size=(10,))</p>"},{"location":"api/#torchfbm.generators.fbm","title":"<code>fbm(n, H, size=(1,), method='davies_harte', device='cpu', dtype=torch.float32, seed=None, return_numpy=False)</code>","text":"<p>Generate Fractional Brownian Motion paths.</p> <p>Based on Mandelbrot &amp; Van Ness (1968).</p> <p>Fractional Brownian Motion \\(B_H(t)\\) is a continuous-time Gaussian process with:</p> <ul> <li>\\(B_H(0) = 0\\)</li> <li>\\(\\mathbb{E}[B_H(t)] = 0\\)</li> <li>\\(\\text{Cov}(B_H(s), B_H(t)) = \\frac{1}{2}(|s|^{2H} + |t|^{2H} - |t-s|^{2H})\\)</li> </ul> <p>The Hurst exponent \\(H \\in (0, 1)\\) controls the path regularity:</p> <ul> <li>\\(H &lt; 0.5\\): Rough paths, anti-persistent (mean-reverting)</li> <li>\\(H = 0.5\\): Standard Brownian motion</li> <li>\\(H &gt; 0.5\\): Smooth paths, persistent (trending)</li> </ul> <p>The fBm path is constructed by cumulative summation of fGn:</p> \\[B_H(t_k) = \\sum_{i=1}^{k} X_i \\quad \\text{where } X \\sim \\text{fGn}(H)\\] <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of increments. Output path has \\(n+1\\) points (includes \\(B_H(0)=0\\)).</p> required <code>H</code> <code>float</code> <p>Hurst exponent in \\((0, 1)\\). Automatically clamped to \\([0.01, 0.99]\\).</p> required <code>size</code> <code>tuple</code> <p>Batch dimensions. Output shape will be <code>(*size, n+1)</code>.</p> <code>(1,)</code> <code>method</code> <code>str</code> <p>Generation method: - 'davies_harte': Fast FFT-based, \\(O(n \\log n)\\) (default) - 'cholesky': Exact but slow, \\(O(n^3)\\)</p> <code>'davies_harte'</code> <code>device</code> <code>str</code> <p>Torch device ('cpu' or 'cuda').</p> <code>'cpu'</code> <code>dtype</code> <code>dtype</code> <p>Data type (torch.float32 or torch.float64).</p> <code>float32</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility.</p> <code>None</code> <code>return_numpy</code> <code>bool</code> <p>If True, returns NumPy array instead of torch.Tensor.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>fBm paths with shape <code>(*size, n+1)</code>. First element is always 0.</p> Example <p>path = fbm(n=1000, H=0.7, size=(10,), seed=42) assert path.shape == (10, 1001) assert (path[:, 0] == 0).all()  # Starts at zero</p> Source code in <code>torchfbm/generators.py</code> <pre><code>def fbm(\n    n: int,\n    H: float,\n    size: tuple = (1,),\n    method: str = \"davies_harte\",\n    device: str = \"cpu\",\n    dtype: torch.dtype = torch.float32,\n    seed: int = None,\n    return_numpy: bool = False,\n) -&gt; torch.Tensor:\n    \"\"\"Generate Fractional Brownian Motion paths.\n\n    Based on Mandelbrot &amp; Van Ness (1968).\n\n    Fractional Brownian Motion $B_H(t)$ is a continuous-time Gaussian process with:\n\n    - $B_H(0) = 0$\n    - $\\\\mathbb{E}[B_H(t)] = 0$\n    - $\\\\text{Cov}(B_H(s), B_H(t)) = \\\\frac{1}{2}(|s|^{2H} + |t|^{2H} - |t-s|^{2H})$\n\n    The Hurst exponent $H \\\\in (0, 1)$ controls the path regularity:\n\n    - $H &lt; 0.5$: Rough paths, anti-persistent (mean-reverting)\n    - $H = 0.5$: Standard Brownian motion\n    - $H &gt; 0.5$: Smooth paths, persistent (trending)\n\n    The fBm path is constructed by cumulative summation of fGn:\n\n    $$B_H(t_k) = \\\\sum_{i=1}^{k} X_i \\\\quad \\\\text{where } X \\\\sim \\\\text{fGn}(H)$$\n\n    Args:\n        n: Number of increments. Output path has $n+1$ points (includes $B_H(0)=0$).\n        H: Hurst exponent in $(0, 1)$. Automatically clamped to $[0.01, 0.99]$.\n        size: Batch dimensions. Output shape will be `(*size, n+1)`.\n        method: Generation method:\n            - 'davies_harte': Fast FFT-based, $O(n \\\\log n)$ (default)\n            - 'cholesky': Exact but slow, $O(n^3)$\n        device: Torch device ('cpu' or 'cuda').\n        dtype: Data type (torch.float32 or torch.float64).\n        seed: Random seed for reproducibility.\n        return_numpy: If True, returns NumPy array instead of torch.Tensor.\n\n    Returns:\n        fBm paths with shape `(*size, n+1)`. First element is always 0.\n\n    Example:\n        &gt;&gt;&gt; path = fbm(n=1000, H=0.7, size=(10,), seed=42)\n        &gt;&gt;&gt; assert path.shape == (10, 1001)\n        &gt;&gt;&gt; assert (path[:, 0] == 0).all()  # Starts at zero\n    \"\"\"\n    H = max(0.01, min(H, 0.99))\n\n    if method == \"cholesky\":\n        func = generate_cholesky\n    else:\n        func = generate_davies_harte\n\n    fgn = func(n, H, size, device=device, dtype=dtype, seed=seed, return_numpy=False)\n\n    zeros = torch.zeros(*size, 1, device=device, dtype=dtype)\n    result = torch.cat([zeros, torch.cumsum(fgn, dim=-1)], dim=-1)\n    return result.cpu().numpy() if return_numpy else result\n</code></pre>"},{"location":"api/#torchfbm.generators.generate_cholesky","title":"<code>generate_cholesky(n, H, size=(1,), device='cpu', dtype=torch.float32, seed=None, return_numpy=False)</code>","text":"<p>Generate fractional Gaussian noise using Cholesky decomposition.</p> <p>Exact method based on Asmussen &amp; Glynn (2007).</p> <p>This method constructs the full covariance matrix \\(\\Sigma\\) and decomposes it as \\(\\Sigma = LL^T\\) where \\(L\\) is lower triangular. The fGn samples are then:</p> \\[X = L \\cdot Z \\quad \\text{where } Z \\sim \\mathcal{N}(0, I)\\] Complexity <ul> <li>Time: \\(O(n^3)\\) for Cholesky decomposition</li> <li>Space: \\(O(n^2)\\) for covariance matrix storage</li> </ul> Note <p>Exact but computationally expensive for large \\(n\\). Use Davies-Harte method for \\(n &gt; 1000\\). May encounter numerical instability for \\(H\\) close to 0 or 1 with large \\(n\\).</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of samples to generate (length of fGn sequence).</p> required <code>H</code> <code>float</code> <p>Hurst exponent in \\((0, 1)\\).</p> required <code>size</code> <code>tuple</code> <p>Batch dimensions. Output shape will be <code>(*size, n)</code>.</p> <code>(1,)</code> <code>device</code> <code>str</code> <p>Torch device ('cpu' or 'cuda').</p> <code>'cpu'</code> <code>dtype</code> <code>dtype</code> <p>Data type (torch.float32 or torch.float64).</p> <code>float32</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility.</p> <code>None</code> <code>return_numpy</code> <code>bool</code> <p>If True, returns NumPy array instead of torch.Tensor.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Fractional Gaussian noise samples with shape <code>(*size, n)</code>.</p> Example <p>fgn = generate_cholesky(n=100, H=0.7, size=(5,), seed=42) assert fgn.shape == (5, 100)</p> Source code in <code>torchfbm/generators.py</code> <pre><code>def generate_cholesky(\n    n: int,\n    H: float,\n    size: tuple = (1,),\n    device: str = \"cpu\",\n    dtype: torch.dtype = torch.float32,\n    seed: int = None,\n    return_numpy: bool = False,\n) -&gt; torch.Tensor:\n    \"\"\"Generate fractional Gaussian noise using Cholesky decomposition.\n\n    Exact method based on Asmussen &amp; Glynn (2007).\n\n    This method constructs the full covariance matrix $\\\\Sigma$ and decomposes it as\n    $\\\\Sigma = LL^T$ where $L$ is lower triangular. The fGn samples are then:\n\n    $$X = L \\\\cdot Z \\\\quad \\\\text{where } Z \\\\sim \\\\mathcal{N}(0, I)$$\n\n    Complexity:\n        - Time: $O(n^3)$ for Cholesky decomposition\n        - Space: $O(n^2)$ for covariance matrix storage\n\n    Note:\n        Exact but computationally expensive for large $n$. Use Davies-Harte method\n        for $n &gt; 1000$. May encounter numerical instability for $H$ close to 0 or 1\n        with large $n$.\n\n    Args:\n        n: Number of samples to generate (length of fGn sequence).\n        H: Hurst exponent in $(0, 1)$.\n        size: Batch dimensions. Output shape will be `(*size, n)`.\n        device: Torch device ('cpu' or 'cuda').\n        dtype: Data type (torch.float32 or torch.float64).\n        seed: Random seed for reproducibility.\n        return_numpy: If True, returns NumPy array instead of torch.Tensor.\n\n    Returns:\n        Fractional Gaussian noise samples with shape `(*size, n)`.\n\n    Example:\n        &gt;&gt;&gt; fgn = generate_cholesky(n=100, H=0.7, size=(5,), seed=42)\n        &gt;&gt;&gt; assert fgn.shape == (5, 100)\n    \"\"\"\n    device = torch.device(device)\n    generator = torch.Generator(device=device)\n    if seed is not None:\n        generator.manual_seed(seed)\n\n    gamma = _autocovariance(H, n, device, dtype)\n    idx = torch.arange(n, device=device)\n    lhs = idx.unsqueeze(0)\n    rhs = idx.unsqueeze(1)\n    distance_matrix = torch.abs(lhs - rhs)\n    Sigma = gamma[distance_matrix]\n\n    jitter = 1e-6 * torch.eye(n, device=device, dtype=dtype)\n    try:\n        L = torch.linalg.cholesky(Sigma + jitter)\n    except RuntimeError:\n        L = torch.linalg.cholesky(Sigma + jitter * 10)\n\n    noise = torch.randn(*size, n, device=device, dtype=dtype, generator=generator)\n    result = torch.matmul(noise, L.t())\n    return result.cpu().numpy() if return_numpy else result\n</code></pre>"},{"location":"api/#torchfbm.generators.generate_davies_harte","title":"<code>generate_davies_harte(n, H, size=(1,), device='cpu', dtype=torch.float32, seed=None, return_numpy=False)</code>","text":"<p>Generate fractional Gaussian noise using the Davies-Harte method.</p> <p>Based on Davies &amp; Harte (1987).</p> <p>This FFT-based method embeds the Toeplitz covariance matrix into a circulant matrix, enabling efficient spectral factorization. The algorithm:</p> <ol> <li>Construct circulant embedding: \\(c = [\\gamma_0, ..., \\gamma_{n-1}, \\gamma_{n-2}, ..., \\gamma_1]\\)</li> <li>Compute eigenvalues via FFT: \\(\\lambda = \\text{FFT}(c)\\)</li> <li>Generate complex noise: \\(W \\sim \\mathcal{CN}(0, I)\\)</li> <li>Apply spectral factorization: \\(X = \\text{IFFT}(\\sqrt{\\lambda} \\cdot W)\\)</li> </ol> Complexity <ul> <li>Time: \\(O(n \\log n)\\) via FFT</li> <li>Space: \\(O(n)\\)</li> </ul> Warning <p>For some combinations of \\(H\\) and \\(n\\), the circulant embedding may have negative eigenvalues. These are clamped to zero with a warning. For exact results in such cases, use the Cholesky method.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of samples to generate (length of fGn sequence).</p> required <code>H</code> <code>float</code> <p>Hurst exponent in \\((0, 1)\\).</p> required <code>size</code> <code>tuple</code> <p>Batch dimensions. Output shape will be <code>(*size, n)</code>.</p> <code>(1,)</code> <code>device</code> <code>str</code> <p>Torch device ('cpu' or 'cuda').</p> <code>'cpu'</code> <code>dtype</code> <code>dtype</code> <p>Data type (torch.float32 or torch.float64).</p> <code>float32</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility.</p> <code>None</code> <code>return_numpy</code> <code>bool</code> <p>If True, returns NumPy array instead of torch.Tensor.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Fractional Gaussian noise samples with shape <code>(*size, n)</code>.</p> Example <p>fgn = generate_davies_harte(n=10000, H=0.7, size=(100,), seed=42) assert fgn.shape == (100, 10000)</p> Source code in <code>torchfbm/generators.py</code> <pre><code>def generate_davies_harte(\n    n: int,\n    H: float,\n    size: tuple = (1,),\n    device: str = \"cpu\",\n    dtype: torch.dtype = torch.float32,\n    seed: int = None,\n    return_numpy: bool = False,\n) -&gt; torch.Tensor:\n    \"\"\"Generate fractional Gaussian noise using the Davies-Harte method.\n\n    Based on Davies &amp; Harte (1987).\n\n    This FFT-based method embeds the Toeplitz covariance matrix into a circulant\n    matrix, enabling efficient spectral factorization. The algorithm:\n\n    1. Construct circulant embedding: $c = [\\\\gamma_0, ..., \\\\gamma_{n-1}, \\\\gamma_{n-2}, ..., \\\\gamma_1]$\n    2. Compute eigenvalues via FFT: $\\\\lambda = \\\\text{FFT}(c)$\n    3. Generate complex noise: $W \\\\sim \\\\mathcal{CN}(0, I)$\n    4. Apply spectral factorization: $X = \\\\text{IFFT}(\\\\sqrt{\\\\lambda} \\\\cdot W)$\n\n    Complexity:\n        - Time: $O(n \\\\log n)$ via FFT\n        - Space: $O(n)$\n\n    Warning:\n        For some combinations of $H$ and $n$, the circulant embedding may have\n        negative eigenvalues. These are clamped to zero with a warning. For exact\n        results in such cases, use the Cholesky method.\n\n    Args:\n        n: Number of samples to generate (length of fGn sequence).\n        H: Hurst exponent in $(0, 1)$.\n        size: Batch dimensions. Output shape will be `(*size, n)`.\n        device: Torch device ('cpu' or 'cuda').\n        dtype: Data type (torch.float32 or torch.float64).\n        seed: Random seed for reproducibility.\n        return_numpy: If True, returns NumPy array instead of torch.Tensor.\n\n    Returns:\n        Fractional Gaussian noise samples with shape `(*size, n)`.\n\n    Example:\n        &gt;&gt;&gt; fgn = generate_davies_harte(n=10000, H=0.7, size=(100,), seed=42)\n        &gt;&gt;&gt; assert fgn.shape == (100, 10000)\n    \"\"\"\n    device = torch.device(device)\n    generator = torch.Generator(device=device)\n    if seed is not None:\n        generator.manual_seed(seed)\n\n    gamma = _autocovariance(H, n, device, dtype)\n\n    # Circulant embedding: [gamma_0, ..., gamma_{n-1}, gamma_{n-2}, ..., gamma_1]\n    row = torch.cat([gamma, gamma[1:-1].flip(0)])\n    M = row.shape[0]\n\n    # FFT (Real to Complex)\n    lambdas = torch.fft.fft(row).real\n    if torch.any(lambdas &lt; 0):\n        warnings.warn(\n            \"Negative eigenvalues encountered in Davies-Harte method, but zeroed out. \"\n            \"Results may be inaccurate. Consider using 'cholesky' method for exact results.\"\n        )\n        lambdas = torch.clamp(lambdas, min=0.0)\n\n    # Generate Complex White Noise with specific generator/dtype\n    rng_real = torch.randn(*size, M, device=device, dtype=dtype, generator=generator)\n    rng_imag = torch.randn(*size, M, device=device, dtype=dtype, generator=generator)\n    complex_noise = torch.complex(rng_real, rng_imag)\n\n    scale = torch.sqrt(lambdas / M)\n    fft_noise = complex_noise * scale\n\n    simulation = torch.fft.ifft(fft_noise) * M\n    result = simulation.real[..., :n]\n    return result.cpu().numpy() if return_numpy else result\n</code></pre>"},{"location":"api/#estimators","title":"Estimators","text":"<p>Hurst parameter estimation using various statistical methods.</p>"},{"location":"api/#torchfbm.estimators","title":"<code>torchfbm.estimators</code>","text":"<p>Hurst exponent estimation methods.</p> <p>This module provides differentiable estimators for the Hurst exponent, enabling end-to-end training of models with fBm-related objectives.</p> Example <p>from torchfbm import fbm, estimate_hurst path = fbm(n=5000, H=0.7, size=(100,)) H_est = estimate_hurst(path) print(f\"Estimated H: {H_est.mean():.3f}\")  # Should be ~0.7</p>"},{"location":"api/#torchfbm.estimators.estimate_hurst","title":"<code>estimate_hurst(x, min_lag=2, max_lag=20, assume_path=True, return_numpy=False)</code>","text":"<p>Estimate the Hurst exponent using the variogram method.</p> <p>Based on Mandelbrot (1969) and the classical rescaled range analysis.</p> <p>This method exploits the self-similarity property of fBm. For increments at lag \\(\\tau\\), the variance scales as:</p> \\[\\text{Var}(B_H(t+\\tau) - B_H(t)) \\propto \\tau^{2H}\\] <p>Taking logarithms:</p> \\[\\log(\\text{Var}(\\tau)) = 2H \\cdot \\log(\\tau) + C\\] <p>The Hurst exponent is estimated via linear regression in log-log space.</p> Note <p>This estimator is differentiable and can be used in loss functions for training neural networks with Hurst-regularized outputs.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input time series of shape <code>(batch, time)</code> or <code>(time,)</code>.</p> required <code>min_lag</code> <code>int</code> <p>Minimum lag for variance estimation.</p> <code>2</code> <code>max_lag</code> <code>int</code> <p>Maximum lag for variance estimation.</p> <code>20</code> <code>assume_path</code> <code>bool</code> <p>If True, treats <code>x</code> as fBm path (default). If False, treats <code>x</code> as fGn (increments) and integrates first.</p> <code>True</code> <code>return_numpy</code> <code>bool</code> <p>If True, returns NumPy array instead of torch.Tensor.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Estimated Hurst exponent(s) clamped to \\([0.01, 0.99]\\).</p> <code>Tensor</code> <p>Shape is <code>(batch,)</code> or scalar for 1D input.</p> Example <p>path = fbm(n=5000, H=0.8, size=(50,), seed=42) H_est = estimate_hurst(path, min_lag=2, max_lag=50) assert abs(H_est.mean() - 0.8) &lt; 0.1  # Should be close to 0.8</p> Source code in <code>torchfbm/estimators.py</code> <pre><code>def estimate_hurst(\n    x: torch.Tensor,\n    min_lag: int = 2,\n    max_lag: int = 20,\n    assume_path: bool = True,\n    return_numpy: bool = False,\n) -&gt; torch.Tensor:\n    \"\"\"Estimate the Hurst exponent using the variogram method.\n\n    Based on Mandelbrot (1969) and the classical rescaled range analysis.\n\n    This method exploits the self-similarity property of fBm. For increments\n    at lag $\\\\tau$, the variance scales as:\n\n    $$\\\\text{Var}(B_H(t+\\\\tau) - B_H(t)) \\\\propto \\\\tau^{2H}$$\n\n    Taking logarithms:\n\n    $$\\\\log(\\\\text{Var}(\\\\tau)) = 2H \\\\cdot \\\\log(\\\\tau) + C$$\n\n    The Hurst exponent is estimated via linear regression in log-log space.\n\n    Note:\n        This estimator is differentiable and can be used in loss functions\n        for training neural networks with Hurst-regularized outputs.\n\n    Args:\n        x: Input time series of shape `(batch, time)` or `(time,)`.\n        min_lag: Minimum lag for variance estimation.\n        max_lag: Maximum lag for variance estimation.\n        assume_path: If True, treats `x` as fBm path (default).\n            If False, treats `x` as fGn (increments) and integrates first.\n        return_numpy: If True, returns NumPy array instead of torch.Tensor.\n\n    Returns:\n        Estimated Hurst exponent(s) clamped to $[0.01, 0.99]$.\n        Shape is `(batch,)` or scalar for 1D input.\n\n    Example:\n        &gt;&gt;&gt; path = fbm(n=5000, H=0.8, size=(50,), seed=42)\n        &gt;&gt;&gt; H_est = estimate_hurst(path, min_lag=2, max_lag=50)\n        &gt;&gt;&gt; assert abs(H_est.mean() - 0.8) &lt; 0.1  # Should be close to 0.8\n    \"\"\"\n    # Normalize to zero mean, unit variance for numerical stability\n    x = (x - x.mean(dim=-1, keepdim=True)) / (x.std(dim=-1, keepdim=True) + 1e-6)\n\n    # Integrate fGn to fBm path if needed\n    if not assume_path:\n        x = torch.cumsum(x, dim=-1)\n\n    lags = torch.arange(min_lag, max_lag, device=x.device)\n    variances = []\n\n    for lag in lags:\n        # Compute increment variance at each scale\n        if x.size(-1) &lt;= lag:\n            break\n        increments = x[..., lag:] - x[..., :-lag]\n        var = increments.var(dim=-1)\n        variances.append(var)\n\n    # Stack variances: Shape (Num_Lags, Batch)\n    variances = torch.stack(variances, dim=0)\n\n    # Log-log regression: log(Var) = 2H * log(lag) + C\n    y = torch.log(variances + 1e-8)\n    X = (\n        torch.log(lags[: variances.size(0)].float()).unsqueeze(1).expand(-1, x.shape[0])\n    )  # (Num_Lags, Batch)\n\n    # Least squares slope estimation\n    X_mean = X.mean(dim=0)\n    y_mean = y.mean(dim=0)\n\n    numerator = ((X - X_mean) * (y - y_mean)).sum(dim=0)\n    denominator = ((X - X_mean) ** 2).sum(dim=0)\n\n    slope = numerator / (denominator + 1e-8)\n\n    # Slope = 2H, so H = Slope / 2\n    H_est = slope / 2.0\n\n    result = torch.clamp(H_est, 0.01, 0.99)\n    return result.cpu().numpy() if return_numpy else result\n</code></pre>"},{"location":"api/#dfa","title":"DFA","text":"<p>Detrended Fluctuation Analysis for detecting long-range correlations.</p>"},{"location":"api/#torchfbm.dfa","title":"<code>torchfbm.dfa</code>","text":"<p>Detrended Fluctuation Analysis (DFA) for scaling exponent estimation.</p> <p>This module provides GPU-accelerated DFA for measuring long-range correlations in time series. DFA is particularly useful for non-stationary signals.</p> <p>Based on Peng et al. (1994).</p> Example <p>from torchfbm import fbm from torchfbm.dfa import dfa path = fbm(n=10000, H=0.7, size=(10,)) alpha = dfa(path) print(f\"DFA exponent: {alpha.mean():.3f}\")  # Should be ~0.7</p>"},{"location":"api/#torchfbm.dfa.dfa","title":"<code>dfa(x, scales=None, order=1, return_alpha=True)</code>","text":"<p>Compute Detrended Fluctuation Analysis scaling exponent.</p> <p>Based on Peng et al. (1994).</p> <p>DFA measures the scaling behavior of the fluctuation function \\(F(s)\\):</p> \\[F(s) \\sim s^{\\alpha}\\] <p>where \\(s\\) is the scale (window size) and \\(\\alpha\\) is the scaling exponent.</p> <p>For fBm, \\(\\alpha = H\\) (the Hurst exponent). The relationship between DFA exponent and correlation structure:</p> <ul> <li>\\(\\alpha &lt; 0.5\\): Anti-correlated (mean-reverting)</li> <li>\\(\\alpha = 0.5\\): Uncorrelated (white noise)</li> <li>\\(0.5 &lt; \\alpha &lt; 1\\): Long-range correlated</li> <li>\\(\\alpha = 1\\): \\(1/f\\) noise (pink noise)</li> <li>\\(\\alpha &gt; 1\\): Non-stationary, unbounded</li> </ul> Algorithm <ol> <li>Compute profile: \\(y(k) = \\sum_{i=1}^{k}(x_i - \\bar{x})\\)</li> <li>Divide into segments of size \\(s\\)</li> <li>Fit polynomial trend in each segment</li> <li>Compute RMS of detrended fluctuations: \\(F(s)\\)</li> <li>Regress \\(\\log F(s)\\) vs \\(\\log s\\) to get \\(\\alpha\\)</li> </ol> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Time series tensor of shape <code>(batch, time)</code> or <code>(time,)</code>.</p> required <code>scales</code> <code>list</code> <p>List of window sizes. If None, uses 20 log-spaced scales.</p> <code>None</code> <code>order</code> <code>int</code> <p>Polynomial order for detrending: - 1: Linear (DFA1, removes linear trends) - 2: Quadratic (DFA2, removes parabolic trends) - 3: Cubic (DFA3)</p> <code>1</code> <code>return_alpha</code> <code>bool</code> <p>If True, returns scaling exponent \\(\\alpha\\). If False, returns tuple <code>(F, scales)</code> with raw fluctuation function.</p> <code>True</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>If <code>return_alpha=True</code>: Scaling exponent(s) of shape <code>(batch,)</code>.</p> <code>Tensor</code> <p>If <code>return_alpha=False</code>: Tuple of (fluctuation tensor, scales array).</p> Example <p>path = fbm(n=10000, H=0.8, size=(20,), seed=42) alpha = dfa(path, order=2) assert abs(alpha.mean() - 0.8) &lt; 0.1</p> Source code in <code>torchfbm/dfa.py</code> <pre><code>def dfa(\n    x: torch.Tensor,\n    scales: list = None,\n    order: int = 1,\n    return_alpha: bool = True,\n) -&gt; torch.Tensor:\n    \"\"\"Compute Detrended Fluctuation Analysis scaling exponent.\n\n    Based on Peng et al. (1994).\n\n    DFA measures the scaling behavior of the fluctuation function $F(s)$:\n\n    $$F(s) \\\\sim s^{\\\\alpha}$$\n\n    where $s$ is the scale (window size) and $\\\\alpha$ is the scaling exponent.\n\n    For fBm, $\\\\alpha = H$ (the Hurst exponent). The relationship between DFA\n    exponent and correlation structure:\n\n    - $\\\\alpha &lt; 0.5$: Anti-correlated (mean-reverting)\n    - $\\\\alpha = 0.5$: Uncorrelated (white noise)\n    - $0.5 &lt; \\\\alpha &lt; 1$: Long-range correlated\n    - $\\\\alpha = 1$: $1/f$ noise (pink noise)\n    - $\\\\alpha &gt; 1$: Non-stationary, unbounded\n\n    Algorithm:\n        1. Compute profile: $y(k) = \\\\sum_{i=1}^{k}(x_i - \\\\bar{x})$\n        2. Divide into segments of size $s$\n        3. Fit polynomial trend in each segment\n        4. Compute RMS of detrended fluctuations: $F(s)$\n        5. Regress $\\\\log F(s)$ vs $\\\\log s$ to get $\\\\alpha$\n\n    Args:\n        x: Time series tensor of shape `(batch, time)` or `(time,)`.\n        scales: List of window sizes. If None, uses 20 log-spaced scales.\n        order: Polynomial order for detrending:\n            - 1: Linear (DFA1, removes linear trends)\n            - 2: Quadratic (DFA2, removes parabolic trends)\n            - 3: Cubic (DFA3)\n        return_alpha: If True, returns scaling exponent $\\\\alpha$.\n            If False, returns tuple `(F, scales)` with raw fluctuation function.\n\n    Returns:\n        If `return_alpha=True`: Scaling exponent(s) of shape `(batch,)`.\n        If `return_alpha=False`: Tuple of (fluctuation tensor, scales array).\n\n    Example:\n        &gt;&gt;&gt; path = fbm(n=10000, H=0.8, size=(20,), seed=42)\n        &gt;&gt;&gt; alpha = dfa(path, order=2)\n        &gt;&gt;&gt; assert abs(alpha.mean() - 0.8) &lt; 0.1\n    \"\"\"\n    # Handle 1D input\n    if x.dim() == 1:\n        x = x.unsqueeze(0)\n\n    device = x.device\n    N = x.shape[1]\n\n    # Compute profile: y(k) = cumsum(x - mean)\n    y = torch.cumsum(x - x.mean(dim=1, keepdim=True), dim=1)\n\n    # Default: 20 logarithmically-spaced scales\n    if scales is None:\n        min_scale = order + 2\n        max_scale = N // 4\n        scales = np.unique(\n            np.logspace(np.log10(min_scale), np.log10(max_scale), num=20).astype(int)\n        )\n\n    fluctuations = []\n\n    for s in scales:\n        # Segment profile into non-overlapping windows\n        n_segments = N // s\n        limit = n_segments * s\n        y_truncated = y[:, :limit]\n        y_segmented = y_truncated.view(x.shape[0], n_segments, s)\n\n        # Batched polynomial detrending via pseudo-inverse\n        t = torch.arange(s, device=device, dtype=x.dtype)\n        X = torch.stack([t**k for k in range(order + 1)], dim=1)\n\n        try:\n            coeffs_op = torch.linalg.pinv(X)\n        except:\n            coeffs_op = torch.linalg.inv(X.T @ X) @ X.T\n\n        beta = torch.matmul(y_segmented, coeffs_op.t())\n        trend = torch.matmul(beta, X.t())\n\n        # RMS of detrended fluctuations\n        rms = torch.sqrt(torch.mean((y_segmented - trend) ** 2, dim=2))\n        F_s = torch.mean(rms, dim=1)\n        fluctuations.append(F_s)\n\n    # Stack fluctuations: (Batch, Num_Scales)\n    F = torch.stack(fluctuations, dim=1)\n\n    if not return_alpha:\n        return F, scales\n\n    # Log-log regression: log(F) = alpha * log(s) + C\n    log_F = torch.log(F)\n    log_scales = torch.log(torch.tensor(scales, device=device, dtype=x.dtype))\n    S_xx = torch.var(log_scales, unbiased=False)\n    mean_x = torch.mean(log_scales)\n    mean_y = torch.mean(log_F, dim=1, keepdim=True)\n\n    # Broadcast subtraction\n    S_xy = torch.mean((log_scales - mean_x) * (log_F - mean_y), dim=1)\n\n    alpha = S_xy / S_xx\n    return alpha\n</code></pre>"},{"location":"api/#analysis","title":"Analysis","text":"<p>Statistical analysis utilities for fBm/fGn data.</p>"},{"location":"api/#torchfbm.analysis","title":"<code>torchfbm.analysis</code>","text":"<p>Analysis utilities for Fractional Brownian Motion.</p> <p>This module provides tools for analyzing and visualizing fBm/fGn processes, including covariance matrix construction, autocorrelation plotting, and spectral analysis.</p> Example <p>from torchfbm.analysis import covariance_matrix, spectral_scaling_factor cov = covariance_matrix(n=100, H=0.7) freqs = torch.linspace(0.01, 0.5, 50) scaling = spectral_scaling_factor(freqs, H=0.7)</p>"},{"location":"api/#torchfbm.analysis.covariance_matrix","title":"<code>covariance_matrix(n, H, device='cpu', return_numpy=False)</code>","text":"<p>Construct the exact autocovariance matrix for fractional Gaussian noise.</p> <p>Builds the symmetric Toeplitz covariance matrix \\(\\Sigma\\) for fGn, where each entry \\(\\Sigma_{ij} = \\gamma(|i-j|)\\) is determined by the autocovariance function.</p> <p>Based on Mandelbrot &amp; Van Ness (1968).</p> <p>The autocovariance function for fGn is:</p> \\[\\gamma(k) = \\frac{1}{2}\\left(|k+1|^{2H} - 2|k|^{2H} + |k-1|^{2H}\\right)\\] <p>This matrix is positive semi-definite and can be Cholesky decomposed for exact fGn generation.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Size of the covariance matrix (n \u00d7 n).</p> required <code>H</code> <code>float</code> <p>Hurst exponent in \\((0, 1)\\). Controls the correlation structure: - \\(H &lt; 0.5\\): Anti-persistent (negative correlations) - \\(H = 0.5\\): Standard Brownian motion (independent increments) - \\(H &gt; 0.5\\): Persistent (positive correlations)</p> required <code>device</code> <code>str</code> <p>Torch device for computation ('cpu' or 'cuda').</p> <code>'cpu'</code> <code>return_numpy</code> <code>bool</code> <p>If True, returns a NumPy array instead of torch.Tensor.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Symmetric positive semi-definite Toeplitz matrix of shape \\((n, n)\\).</p> Example <p>cov = covariance_matrix(n=50, H=0.8) assert cov.shape == (50, 50) assert torch.allclose(cov, cov.T)  # Symmetric</p> Source code in <code>torchfbm/analysis.py</code> <pre><code>def covariance_matrix(\n    n: int, H: float, device: str = \"cpu\", return_numpy: bool = False\n) -&gt; torch.Tensor:\n    \"\"\"Construct the exact autocovariance matrix for fractional Gaussian noise.\n\n    Builds the symmetric Toeplitz covariance matrix $\\\\Sigma$ for fGn, where each\n    entry $\\\\Sigma_{ij} = \\\\gamma(|i-j|)$ is determined by the autocovariance function.\n\n    Based on Mandelbrot &amp; Van Ness (1968).\n\n    The autocovariance function for fGn is:\n\n    $$\\\\gamma(k) = \\\\frac{1}{2}\\\\left(|k+1|^{2H} - 2|k|^{2H} + |k-1|^{2H}\\\\right)$$\n\n    This matrix is positive semi-definite and can be Cholesky decomposed for\n    exact fGn generation.\n\n    Args:\n        n: Size of the covariance matrix (n \u00d7 n).\n        H: Hurst exponent in $(0, 1)$. Controls the correlation structure:\n            - $H &lt; 0.5$: Anti-persistent (negative correlations)\n            - $H = 0.5$: Standard Brownian motion (independent increments)\n            - $H &gt; 0.5$: Persistent (positive correlations)\n        device: Torch device for computation ('cpu' or 'cuda').\n        return_numpy: If True, returns a NumPy array instead of torch.Tensor.\n\n    Returns:\n        Symmetric positive semi-definite Toeplitz matrix of shape $(n, n)$.\n\n    Example:\n        &gt;&gt;&gt; cov = covariance_matrix(n=50, H=0.8)\n        &gt;&gt;&gt; assert cov.shape == (50, 50)\n        &gt;&gt;&gt; assert torch.allclose(cov, cov.T)  # Symmetric\n    \"\"\"\n    from .generators import _autocovariance\n\n    gamma = _autocovariance(H, n, torch.device(device), dtype=torch.float32)\n\n    # Construct Toeplitz matrix from autocovariance\n    idx = torch.arange(n, device=device)\n    distance_matrix = torch.abs(idx.unsqueeze(0) - idx.unsqueeze(1))\n    result = gamma[distance_matrix]\n    return result.cpu().numpy() if return_numpy else result\n</code></pre>"},{"location":"api/#torchfbm.analysis.plot_acf","title":"<code>plot_acf(x, max_lag=100, title='Autocorrelation')</code>","text":"<p>Plot the autocorrelation function of a time series.</p> <p>Computes and visualizes the ACF using FFT-based circular convolution for efficient \\(O(N \\log N)\\) computation.</p> <p>The autocorrelation at lag \\(k\\) is defined as:</p> \\[\\rho(k) = \\frac{\\text{Cov}(X_t, X_{t+k})}{\\text{Var}(X_t)}\\] <p>For fGn with Hurst exponent \\(H\\), the theoretical ACF decays as:</p> \\[\\rho(k) \\sim H(2H-1)|k|^{2H-2} \\quad \\text{as } k \\to \\infty\\] Note <p>Requires matplotlib to be installed separately.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input time series tensor. Last dimension is treated as time axis.</p> required <code>max_lag</code> <code>int</code> <p>Maximum lag to display in the plot.</p> <code>100</code> <code>title</code> <code>str</code> <p>Title for the plot.</p> <code>'Autocorrelation'</code> <p>Returns:</p> Type Description <code>None</code> <p>None. Displays the plot using matplotlib.</p> Example <p>from torchfbm import fbm path = fbm(n=1000, H=0.7, size=(1,)) plot_acf(path.squeeze(), max_lag=50, title=\"fBm ACF (H=0.7)\")</p> Source code in <code>torchfbm/analysis.py</code> <pre><code>def plot_acf(x: torch.Tensor, max_lag: int = 100, title: str = \"Autocorrelation\") -&gt; None:\n    \"\"\"Plot the autocorrelation function of a time series.\n\n    Computes and visualizes the ACF using FFT-based circular convolution for\n    efficient $O(N \\\\log N)$ computation.\n\n    The autocorrelation at lag $k$ is defined as:\n\n    $$\\\\rho(k) = \\\\frac{\\\\text{Cov}(X_t, X_{t+k})}{\\\\text{Var}(X_t)}$$\n\n    For fGn with Hurst exponent $H$, the theoretical ACF decays as:\n\n    $$\\\\rho(k) \\\\sim H(2H-1)|k|^{2H-2} \\\\quad \\\\text{as } k \\\\to \\\\infty$$\n\n    Note:\n        Requires matplotlib to be installed separately.\n\n    Args:\n        x: Input time series tensor. Last dimension is treated as time axis.\n        max_lag: Maximum lag to display in the plot.\n        title: Title for the plot.\n\n    Returns:\n        None. Displays the plot using matplotlib.\n\n    Example:\n        &gt;&gt;&gt; from torchfbm import fbm\n        &gt;&gt;&gt; path = fbm(n=1000, H=0.7, size=(1,))\n        &gt;&gt;&gt; plot_acf(path.squeeze(), max_lag=50, title=\"fBm ACF (H=0.7)\")\n    \"\"\"\n    try:\n        import matplotlib.pyplot as plt\n    except ImportError:\n        print(\"Matplotlib not installed.\")\n        return\n\n    # FFT-based ACF computation\n    n = x.shape[-1]\n    x_centered = x - x.mean()\n    next_pow2 = 2 ** (2 * n - 1).bit_length()\n    # Zero-pad to next power of 2 for efficiency and avoid circular correlation\n    x_padded = torch.nn.functional.pad(x_centered, (0, next_pow2 - n))\n    fft = torch.fft.fft(x_padded)\n\n    power = fft * fft.conj()\n    acf = torch.fft.ifft(power).real\n    acf = acf[:max_lag]\n    acf = acf / acf[0]\n    acf_np = acf.detach().cpu().numpy()\n    plt.figure(figsize=(10, 4))\n    plt.bar(range(len(acf_np)), acf_np, width=0.5)\n    plt.title(title)\n    plt.xlabel(\"Lag\")\n    plt.ylabel(\"Correlation\")\n    plt.grid(True, alpha=0.3)\n    plt.show()\n</code></pre>"},{"location":"api/#torchfbm.analysis.spectral_scaling_factor","title":"<code>spectral_scaling_factor(f, H, return_numpy=False)</code>","text":"<p>Compute the spectral scaling factor for fBm spectral synthesis.</p> <p>Returns the amplitude scaling \\(A(f)\\) needed to synthesize fBm via spectral methods. Based on the \\(1/f^\\beta\\) power spectral density law.</p> <p>Based on Flandrin (1989) and Mandelbrot &amp; Van Ness (1968).</p> <p>The power spectral density of fBm scales as:</p> \\[S(f) \\propto \\frac{1}{|f|^\\beta} \\quad \\text{where } \\beta = 2H + 1\\] <p>The amplitude scaling is therefore:</p> \\[A(f) = \\frac{1}{|f|^{(H + 0.5)}}\\] <p>This is used in the spectral synthesis method where fBm is generated by filtering white noise with this frequency-dependent amplitude.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Tensor</code> <p>Frequency tensor. Can be any shape; scaling is applied element-wise.</p> required <code>H</code> <code>float</code> <p>Hurst exponent in \\((0, 1)\\).</p> required <code>return_numpy</code> <code>bool</code> <p>If True, returns a NumPy array instead of torch.Tensor.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Spectral scaling factors with same shape as input <code>f</code>.</p> <code>Tensor</code> <p>DC component (\\(f=0\\)) is set to zero.</p> Example <p>freqs = torch.linspace(0.01, 0.5, 100) scaling = spectral_scaling_factor(freqs, H=0.7)</p> Source code in <code>torchfbm/analysis.py</code> <pre><code>def spectral_scaling_factor(\n    f: torch.Tensor, H: float, return_numpy: bool = False\n) -&gt; torch.Tensor:\n    \"\"\"Compute the spectral scaling factor for fBm spectral synthesis.\n\n    Returns the amplitude scaling $A(f)$ needed to synthesize fBm via spectral\n    methods. Based on the $1/f^\\\\beta$ power spectral density law.\n\n    Based on Flandrin (1989) and Mandelbrot &amp; Van Ness (1968).\n\n    The power spectral density of fBm scales as:\n\n    $$S(f) \\\\propto \\\\frac{1}{|f|^\\\\beta} \\\\quad \\\\text{where } \\\\beta = 2H + 1$$\n\n    The amplitude scaling is therefore:\n\n    $$A(f) = \\\\frac{1}{|f|^{(H + 0.5)}}$$\n\n    This is used in the spectral synthesis method where fBm is generated by\n    filtering white noise with this frequency-dependent amplitude.\n\n    Args:\n        f: Frequency tensor. Can be any shape; scaling is applied element-wise.\n        H: Hurst exponent in $(0, 1)$.\n        return_numpy: If True, returns a NumPy array instead of torch.Tensor.\n\n    Returns:\n        Spectral scaling factors with same shape as input `f`.\n        DC component ($f=0$) is set to zero.\n\n    Example:\n        &gt;&gt;&gt; freqs = torch.linspace(0.01, 0.5, 100)\n        &gt;&gt;&gt; scaling = spectral_scaling_factor(freqs, H=0.7)\n        &gt;&gt;&gt; # Use for spectral synthesis: fft_coeffs = white_noise * scaling\n    \"\"\"\n    beta = 2 * H + 1\n    safe_f = torch.where(f == 0, torch.ones_like(f), f)\n    scaling = 1.0 / (torch.abs(safe_f) ** (beta / 2.0))\n    scaling[f == 0] = 0  # Zero DC component\n    return scaling.cpu().numpy() if return_numpy else scaling\n</code></pre>"},{"location":"api/#torchfbm.analysis.spectral_scaling_factor--use-for-spectral-synthesis-fft_coeffs-white_noise-scaling","title":"Use for spectral synthesis: fft_coeffs = white_noise * scaling","text":""},{"location":"api/#processes","title":"Processes","text":"<p>Stochastic processes driven by fractional Brownian motion.</p>"},{"location":"api/#torchfbm.processes","title":"<code>torchfbm.processes</code>","text":"<p>Stochastic processes driven by fractional Brownian motion.</p> <p>This module provides implementations of various stochastic processes that incorporate fractional Brownian motion as the driving noise, enabling simulation of systems with long-range dependence and anomalous diffusion.</p> Processes Included <ul> <li>Fractional Ornstein-Uhlenbeck: Mean-reverting process with memory</li> <li>Geometric fBm: Asset price model with long-range dependence</li> <li>Reflected fBm: Bounded fBm with reflection barriers</li> <li>Fractional Brownian Bridge: fBm conditioned on endpoint</li> <li>Multifractal Random Walk: Volatility clustering model</li> </ul> Example <p>from torchfbm.processes import fractional_ou_process, geometric_fbm</p>"},{"location":"api/#torchfbm.processes--mean-reverting-process-with-memory","title":"Mean-reverting process with memory","text":"<p>ou = fractional_ou_process(1000, H=0.7, theta=0.5, mu=0.0)</p>"},{"location":"api/#torchfbm.processes--asset-prices-with-long-range-dependence","title":"Asset prices with long-range dependence","text":"<p>prices = geometric_fbm(252, H=0.6, mu=0.05, sigma=0.2, s0=100)</p>"},{"location":"api/#torchfbm.processes.fractional_brownian_bridge","title":"<code>fractional_brownian_bridge(n, H, start_val=0.0, end_val=0.0, t_max=1.0, sigma=1.0, size=(1,), method='davies_harte', device='cpu', return_numpy=False)</code>","text":"<p>Simulate a Fractional Brownian Bridge.</p> <p>Generates fBm conditioned on fixed start and end values. Based on the pinning method described in Norros, Valkeila &amp; Virtamo (1999).</p> <p>The bridge is constructed using linear correction of a free fBm path:</p> \\[B^{H,bridge}_t = B^H_t - \\frac{t}{T}(B^H_T - (end - start)) + start\\] <p>This produces a \"rough\" path (for \\(H &lt; 0.5\\)) or \"smooth\" path (for \\(H &gt; 0.5\\)) that is pinned to specific boundary values.</p> Applications <ul> <li>Finance: Modeling prices with known future values (options at expiry)</li> <li>Simulation: Conditioning on observed endpoints</li> <li>Interpolation: Rough path interpolation between data points</li> <li>Testing: Generating paths with known boundary conditions</li> </ul> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of time steps.</p> required <code>H</code> <code>float</code> <p>Hurst parameter in (0, 1).</p> required <code>start_val</code> <code>float</code> <p>Starting value \\(B^{H,bridge}_0\\).</p> <code>0.0</code> <code>end_val</code> <code>float</code> <p>Ending value \\(B^{H,bridge}_T\\).</p> <code>0.0</code> <code>t_max</code> <code>float</code> <p>Total time horizon \\(T\\).</p> <code>1.0</code> <code>sigma</code> <code>float</code> <p>Volatility scaling.</p> <code>1.0</code> <code>size</code> <code>tuple</code> <p>Batch dimensions.</p> <code>(1,)</code> <code>method</code> <code>str</code> <p>Generation method ('davies_harte' or 'cholesky').</p> <code>'davies_harte'</code> <code>device</code> <code>str</code> <p>Computation device.</p> <code>'cpu'</code> <code>return_numpy</code> <code>bool</code> <p>If True, returns NumPy array.</p> <code>False</code> <p>Returns:</p> Type Description <p>Tensor of shape <code>(*size, n+1)</code> with bridge paths.</p> Example Source code in <code>torchfbm/processes.py</code> <pre><code>def fractional_brownian_bridge(\n    n: int,\n    H: float,\n    start_val: float = 0.0,\n    end_val: float = 0.0,\n    t_max: float = 1.0,\n    sigma: float = 1.0,\n    size: tuple = (1,),\n    method: str = \"davies_harte\",\n    device: str = \"cpu\",\n    return_numpy: bool = False,\n):\n    \"\"\"Simulate a Fractional Brownian Bridge.\n\n    Generates fBm conditioned on fixed start and end values.\n    Based on the pinning method described in Norros, Valkeila &amp; Virtamo (1999).\n\n    The bridge is constructed using linear correction of a free fBm path:\n\n    $$B^{H,bridge}_t = B^H_t - \\\\frac{t}{T}(B^H_T - (end - start)) + start$$\n\n    This produces a \"rough\" path (for $H &lt; 0.5$) or \"smooth\" path (for $H &gt; 0.5$)\n    that is pinned to specific boundary values.\n\n    Applications:\n        - **Finance**: Modeling prices with known future values (options at expiry)\n        - **Simulation**: Conditioning on observed endpoints\n        - **Interpolation**: Rough path interpolation between data points\n        - **Testing**: Generating paths with known boundary conditions\n\n    Args:\n        n: Number of time steps.\n        H: Hurst parameter in (0, 1).\n        start_val: Starting value $B^{H,bridge}_0$.\n        end_val: Ending value $B^{H,bridge}_T$.\n        t_max: Total time horizon $T$.\n        sigma: Volatility scaling.\n        size: Batch dimensions.\n        method: Generation method ('davies_harte' or 'cholesky').\n        device: Computation device.\n        return_numpy: If True, returns NumPy array.\n\n    Returns:\n        Tensor of shape ``(*size, n+1)`` with bridge paths.\n\n    Example:\n        &gt;&gt;&gt; # Bridge from 0 to 1 with rough texture\n        &gt;&gt;&gt; bridge = fractional_brownian_bridge(\n        ...     n=1000, H=0.3, start_val=0.0, end_val=1.0\n        ... )\n        &gt;&gt;&gt; print(bridge[0, 0], bridge[0, -1])  # ~0.0, ~1.0\n    \"\"\"\n    device = torch.device(device)\n\n    # 1. Generate a FREE unconditioned path starting at 0\n    # We use fbm() to handle H-clamping and method selection\n    # shape: (..., n+1)\n    free_path = fbm(n, H, size=size, method=method, device=device, return_numpy=False)\n\n    # 2. Scale the free path to physical time/sigma\n    # Scale factor for variance over time T is T^(2H)\n    # The generator gives us unit step variance.\n    scale_factor = sigma * (t_max / n) ** H\n    free_path = free_path * scale_factor\n\n    # 3. Create the Time Grid\n    # Shape: (1, ..., n+1) to broadcast correctly\n    t_grid = torch.linspace(0, t_max, n + 1, device=device)\n    # Reshape for broadcasting against 'size'\n    # If size=(Batch, ), free_path is (Batch, n+1).\n    # We need t_grid to be (1, n+1) compatible.\n    for _ in range(len(size)):\n        t_grid = t_grid.unsqueeze(0)\n\n    # 4. Calculate the Bridge \"Drift\"\n    # We need to subtract the error at the end.\n    # The free path ends at X_T. We want it to end at (end_val - start_val).\n    # Current endpoint error = free_path[..., -1]\n\n    current_end = free_path[..., -1:]  # Keep dims for broadcast\n    target_displacement = end_val - start_val\n\n    # The correction term is linear interpolation of the error\n    # Correction(t) = (t / T) * (current_end - target_displacement)\n    correction = (t_grid / t_max) * (current_end - target_displacement)\n\n    # 5. Apply correction and shift start\n    bridge = free_path - correction + start_val\n\n    return bridge.cpu().numpy() if return_numpy else bridge\n</code></pre>"},{"location":"api/#torchfbm.processes.fractional_brownian_bridge--bridge-from-0-to-1-with-rough-texture","title":"Bridge from 0 to 1 with rough texture","text":"<p>bridge = fractional_brownian_bridge( ...     n=1000, H=0.3, start_val=0.0, end_val=1.0 ... ) print(bridge[0, 0], bridge[0, -1])  # ~0.0, ~1.0</p>"},{"location":"api/#torchfbm.processes.fractional_ou_process","title":"<code>fractional_ou_process(n, H, theta=0.5, mu=0.0, sigma=1.0, dt=0.01, size=(1,), method='davies_harte', device='cpu', dtype=torch.float32, return_numpy=False)</code>","text":"<p>Simulate a Fractional Ornstein-Uhlenbeck (fOU) process.</p> <p>Based on Cheridito, Kawaguchi &amp; Maejima (2003).</p> <p>The fOU process is defined by the stochastic differential equation:</p> \\[dX_t = \\theta(\\mu - X_t)dt + \\sigma dB^H_t\\] <p>where: - \\(\\theta\\) is the mean-reversion speed - \\(\\mu\\) is the long-term mean - \\(\\sigma\\) is the volatility - \\(B^H_t\\) is fractional Brownian motion with Hurst parameter \\(H\\)</p> Properties <ul> <li>H &gt; 0.5: Persistent memory, slower mean-reversion than standard OU</li> <li>H = 0.5: Reduces to standard OU process</li> <li>H &lt; 0.5: Anti-persistent, faster mean-reversion</li> </ul> <p>The process is stationary and mean-reverting, but unlike standard OU, it exhibits long-range dependence when \\(H \\neq 0.5\\).</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of time steps.</p> required <code>H</code> <code>float</code> <p>Hurst parameter in (0, 1). Controls memory persistence.</p> required <code>theta</code> <code>float</code> <p>Mean-reversion speed. Higher values = faster reversion.</p> <code>0.5</code> <code>mu</code> <code>float</code> <p>Long-term mean level.</p> <code>0.0</code> <code>sigma</code> <code>float</code> <p>Volatility coefficient.</p> <code>1.0</code> <code>dt</code> <code>float</code> <p>Time step size.</p> <code>0.01</code> <code>size</code> <code>tuple</code> <p>Batch dimensions for multiple sample paths.</p> <code>(1,)</code> <code>method</code> <code>str</code> <p>Generation method, either 'davies_harte' (fast) or 'cholesky' (exact).</p> <code>'davies_harte'</code> <code>device</code> <code>str</code> <p>Computation device ('cpu' or 'cuda').</p> <code>'cpu'</code> <code>dtype</code> <code>dtype</code> <p>Data type for tensors.</p> <code>float32</code> <code>return_numpy</code> <code>bool</code> <p>If True, returns NumPy array.</p> <code>False</code> <p>Returns:</p> Type Description <p>Tensor of shape <code>(*size, n+1)</code> containing the simulated paths.</p> Example Source code in <code>torchfbm/processes.py</code> <pre><code>def fractional_ou_process(\n    n: int,\n    H: float,\n    theta: float = 0.5,\n    mu: float = 0.0,\n    sigma: float = 1.0,\n    dt: float = 0.01,\n    size: tuple = (1,),\n    method: str = \"davies_harte\",\n    device: str = \"cpu\",\n    dtype: torch.dtype = torch.float32,\n    return_numpy: bool = False,\n):\n    \"\"\"Simulate a Fractional Ornstein-Uhlenbeck (fOU) process.\n\n    Based on Cheridito, Kawaguchi &amp; Maejima (2003).\n\n    The fOU process is defined by the stochastic differential equation:\n\n    $$dX_t = \\\\theta(\\\\mu - X_t)dt + \\\\sigma dB^H_t$$\n\n    where:\n    - $\\\\theta$ is the mean-reversion speed\n    - $\\\\mu$ is the long-term mean\n    - $\\\\sigma$ is the volatility\n    - $B^H_t$ is fractional Brownian motion with Hurst parameter $H$\n\n    Properties:\n        - **H &gt; 0.5**: Persistent memory, slower mean-reversion than standard OU\n        - **H = 0.5**: Reduces to standard OU process\n        - **H &lt; 0.5**: Anti-persistent, faster mean-reversion\n\n    The process is stationary and mean-reverting, but unlike standard OU,\n    it exhibits long-range dependence when $H \\\\neq 0.5$.\n\n    Args:\n        n: Number of time steps.\n        H: Hurst parameter in (0, 1). Controls memory persistence.\n        theta: Mean-reversion speed. Higher values = faster reversion.\n        mu: Long-term mean level.\n        sigma: Volatility coefficient.\n        dt: Time step size.\n        size: Batch dimensions for multiple sample paths.\n        method: Generation method, either 'davies_harte' (fast) or 'cholesky' (exact).\n        device: Computation device ('cpu' or 'cuda').\n        dtype: Data type for tensors.\n        return_numpy: If True, returns NumPy array.\n\n    Returns:\n        Tensor of shape ``(*size, n+1)`` containing the simulated paths.\n\n    Example:\n        &gt;&gt;&gt; # Simulate interest rate with memory\n        &gt;&gt;&gt; rates = fractional_ou_process(\n        ...     n=1000, H=0.7, theta=0.1, mu=0.05, sigma=0.01\n        ... )\n    \"\"\"\n    # Clamp H to valid range (0, 1)\n    H = max(0.01, min(H, 0.99))\n\n    if method == \"cholesky\":\n        gen_func = generate_cholesky\n    else:\n        gen_func = generate_davies_harte\n\n    # Generate fGn and scale by dt^H\n    fgn = gen_func(n, H, size, device=device, dtype=dtype, return_numpy=False)\n    noise_term = sigma * fgn * (dt**H)\n\n    # Euler-Maruyama integration\n    x = torch.zeros(*size, n + 1, device=device, dtype=dtype)\n    x[..., 0] = mu  # Start at mean\n\n    drift_factor = 1 - theta * dt\n    drift_constant = theta * mu * dt\n\n    for t in range(n):\n        x[..., t + 1] = x[..., t] * drift_factor + drift_constant + noise_term[..., t]\n\n    return x.cpu().numpy() if return_numpy else x\n</code></pre>"},{"location":"api/#torchfbm.processes.fractional_ou_process--simulate-interest-rate-with-memory","title":"Simulate interest rate with memory","text":"<p>rates = fractional_ou_process( ...     n=1000, H=0.7, theta=0.1, mu=0.05, sigma=0.01 ... )</p>"},{"location":"api/#torchfbm.processes.geometric_fbm","title":"<code>geometric_fbm(n, H, mu=0.05, sigma=0.2, t_max=1.0, s0=100.0, size=(1,), method='davies_harte', device='cpu', dtype=torch.float32, return_numpy=False)</code>","text":"<p>Simulate Geometric Fractional Brownian Motion (GFBm).</p> <p>Generalization of Geometric Brownian Motion with long-range dependence. Based on the framework described in Rogers (1997).</p> <p>The price follows:</p> \\[S_t = S_0 \\exp\\left((\\mu - \\frac{1}{2}\\sigma^2)t + \\sigma B^H_t\\right)\\] <p>where: - \\(S_0\\) is the initial price - \\(\\mu\\) is the drift (expected return) - \\(\\sigma\\) is the volatility - \\(B^H_t\\) is fractional Brownian motion</p> Note <p>Unlike standard GBM, GFBm with \\(H \\neq 0.5\\) admits arbitrage in continuous time. However, it remains useful for modeling observed market properties like volatility clustering and trend persistence.</p> Applications <ul> <li>Finance: Modeling assets with trending behavior (\\(H &gt; 0.5\\))</li> <li>Volatility modeling: Capturing long-memory in volatility</li> <li>Risk analysis: Fat-tailed return distributions</li> </ul> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of time steps.</p> required <code>H</code> <code>float</code> <p>Hurst parameter in (0, 1).</p> required <code>mu</code> <code>float</code> <p>Drift coefficient (annualized return).</p> <code>0.05</code> <code>sigma</code> <code>float</code> <p>Volatility coefficient (annualized).</p> <code>0.2</code> <code>t_max</code> <code>float</code> <p>Total time horizon.</p> <code>1.0</code> <code>s0</code> <code>float</code> <p>Initial price.</p> <code>100.0</code> <code>size</code> <code>tuple</code> <p>Batch dimensions for multiple paths.</p> <code>(1,)</code> <code>method</code> <code>str</code> <p>Generation method ('davies_harte' or 'cholesky').</p> <code>'davies_harte'</code> <code>device</code> <code>str</code> <p>Computation device.</p> <code>'cpu'</code> <code>dtype</code> <code>dtype</code> <p>Data type for tensors.</p> <code>float32</code> <code>return_numpy</code> <code>bool</code> <p>If True, returns NumPy array.</p> <code>False</code> <p>Returns:</p> Type Description <p>Tensor of shape <code>(*size, n+1)</code> containing price paths.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If n &lt;= 0.</p> Example Source code in <code>torchfbm/processes.py</code> <pre><code>def geometric_fbm(\n    n: int,\n    H: float,\n    mu: float = 0.05,\n    sigma: float = 0.2,\n    t_max: float = 1.0,\n    s0: float = 100.0,\n    size: tuple = (1,),\n    method: str = \"davies_harte\",\n    device: str = \"cpu\",\n    dtype: torch.dtype = torch.float32,\n    return_numpy: bool = False,\n):\n    \"\"\"Simulate Geometric Fractional Brownian Motion (GFBm).\n\n    Generalization of Geometric Brownian Motion with long-range dependence.\n    Based on the framework described in Rogers (1997).\n\n    The price follows:\n\n    $$S_t = S_0 \\\\exp\\\\left((\\\\mu - \\\\frac{1}{2}\\\\sigma^2)t + \\\\sigma B^H_t\\\\right)$$\n\n    where:\n    - $S_0$ is the initial price\n    - $\\\\mu$ is the drift (expected return)\n    - $\\\\sigma$ is the volatility\n    - $B^H_t$ is fractional Brownian motion\n\n    Note:\n        Unlike standard GBM, GFBm with $H \\\\neq 0.5$ admits arbitrage in\n        continuous time. However, it remains useful for modeling observed\n        market properties like volatility clustering and trend persistence.\n\n    Applications:\n        - **Finance**: Modeling assets with trending behavior ($H &gt; 0.5$)\n        - **Volatility modeling**: Capturing long-memory in volatility\n        - **Risk analysis**: Fat-tailed return distributions\n\n    Args:\n        n: Number of time steps.\n        H: Hurst parameter in (0, 1).\n        mu: Drift coefficient (annualized return).\n        sigma: Volatility coefficient (annualized).\n        t_max: Total time horizon.\n        s0: Initial price.\n        size: Batch dimensions for multiple paths.\n        method: Generation method ('davies_harte' or 'cholesky').\n        device: Computation device.\n        dtype: Data type for tensors.\n        return_numpy: If True, returns NumPy array.\n\n    Returns:\n        Tensor of shape ``(*size, n+1)`` containing price paths.\n\n    Raises:\n        ValueError: If n &lt;= 0.\n\n    Example:\n        &gt;&gt;&gt; # Simulate 1 year of daily prices\n        &gt;&gt;&gt; prices = geometric_fbm(\n        ...     n=252, H=0.6, mu=0.08, sigma=0.20, s0=100.0\n        ... )\n    \"\"\"\n    if n &lt;= 0:\n        raise ValueError(\"n must be a positive integer\")\n\n    device = torch.device(device)\n\n    t = torch.linspace(0, t_max, n + 1, device=device, dtype=dtype).expand(*size, n + 1)\n\n    # Generate fBm path and scale to time horizon t_max\n    fbm_path = fbm(\n        n, H, size=size, method=method, device=device, dtype=dtype, return_numpy=False\n    )\n    scale_factor = (t_max / n) ** H\n    fbm_path = fbm_path * scale_factor\n\n    # Apply exponential transformation\n    drift = (mu - 0.5 * sigma**2) * t\n    diffusion = sigma * fbm_path\n\n    log_returns = drift + diffusion\n\n    result = s0 * torch.exp(log_returns)\n    return result.cpu().numpy() if return_numpy else result\n</code></pre>"},{"location":"api/#torchfbm.processes.geometric_fbm--simulate-1-year-of-daily-prices","title":"Simulate 1 year of daily prices","text":"<p>prices = geometric_fbm( ...     n=252, H=0.6, mu=0.08, sigma=0.20, s0=100.0 ... )</p>"},{"location":"api/#torchfbm.processes.multifractal_random_walk","title":"<code>multifractal_random_walk(n, H, lambda_sq=0.02, device='cpu')</code>","text":"<p>Simulate a Multifractal Random Walk (MRW).</p> <p>Based on Bacry, Delour &amp; Muzy (2001).</p> <p>The MRW combines fractional noise with stochastic volatility to produce multifractal scaling:</p> \\[X_t = \\sum_{i=1}^{t} \\sigma_i \\epsilon_i\\] <p>where the volatility is:</p> \\[\\sigma_t = \\exp(\\lambda^2 \\omega_t)\\] <p>and \\(\\omega_t\\) is fGn with Hurst parameter \\(H\\), \\(\\epsilon_t\\) is Gaussian noise.</p> <p>The intermittency parameter \\(\\lambda^2\\) controls the strength of volatility clustering: - \\(\\lambda^2 \\approx 0\\): Nearly Gaussian returns - \\(\\lambda^2 &gt; 0\\): Fat tails and volatility clustering - Higher \\(\\lambda^2\\): More extreme events (\"flash crashes\")</p> Properties <ul> <li>Multifractal spectrum depends on both \\(H\\) and \\(\\lambda^2\\)</li> <li>Captures stylized facts of financial returns</li> <li>Long-memory in squared/absolute returns</li> </ul> <p>Parameters:</p> Name Type Description Default <code>n</code> <p>Number of time steps.</p> required <code>H</code> <p>Hurst parameter for the volatility process.</p> required <code>lambda_sq</code> <p>Intermittency coefficient (controls tail fatness).</p> <code>0.02</code> <code>device</code> <p>Computation device.</p> <code>'cpu'</code> <p>Returns:</p> Type Description <p>Tensor of shape <code>(n,)</code> containing the MRW path.</p> Example Source code in <code>torchfbm/processes.py</code> <pre><code>def multifractal_random_walk(n, H, lambda_sq=0.02, device=\"cpu\"):\n    \"\"\"Simulate a Multifractal Random Walk (MRW).\n\n    Based on Bacry, Delour &amp; Muzy (2001).\n\n    The MRW combines fractional noise with stochastic volatility to produce\n    multifractal scaling:\n\n    $$X_t = \\\\sum_{i=1}^{t} \\\\sigma_i \\\\epsilon_i$$\n\n    where the volatility is:\n\n    $$\\\\sigma_t = \\\\exp(\\\\lambda^2 \\\\omega_t)$$\n\n    and $\\\\omega_t$ is fGn with Hurst parameter $H$, $\\\\epsilon_t$ is Gaussian noise.\n\n    The intermittency parameter $\\\\lambda^2$ controls the strength of\n    volatility clustering:\n    - $\\\\lambda^2 \\\\approx 0$: Nearly Gaussian returns\n    - $\\\\lambda^2 &gt; 0$: Fat tails and volatility clustering\n    - Higher $\\\\lambda^2$: More extreme events (\"flash crashes\")\n\n    Properties:\n        - Multifractal spectrum depends on both $H$ and $\\\\lambda^2$\n        - Captures stylized facts of financial returns\n        - Long-memory in squared/absolute returns\n\n    Args:\n        n: Number of time steps.\n        H: Hurst parameter for the volatility process.\n        lambda_sq: Intermittency coefficient (controls tail fatness).\n        device: Computation device.\n\n    Returns:\n        Tensor of shape ``(n,)`` containing the MRW path.\n\n    Example:\n        &gt;&gt;&gt; # Simulate returns with volatility clustering\n        &gt;&gt;&gt; mrw = multifractal_random_walk(1000, H=0.7, lambda_sq=0.02)\n    \"\"\"\n    # 1. Generate fGn for the volatility cone (omega)\n    # The correlation of omega is logarithmic\n    # This is complex to do perfectly, but a proxy is:\n    omega = generate_davies_harte(n, H, device=device)\n\n    # 2. Stochastic Volatility\n    sigma = torch.exp(lambda_sq * omega)\n\n    # 3. The Walk\n    noise = torch.randn(n, device=device)\n    mrw = torch.cumsum(sigma * noise, dim=-1)\n    return mrw\n</code></pre>"},{"location":"api/#torchfbm.processes.multifractal_random_walk--simulate-returns-with-volatility-clustering","title":"Simulate returns with volatility clustering","text":"<p>mrw = multifractal_random_walk(1000, H=0.7, lambda_sq=0.02)</p>"},{"location":"api/#torchfbm.processes.reflected_fbm","title":"<code>reflected_fbm(n, H, lower=-1.0, upper=1.0, mu=0.0, sigma=1.0, t_max=1.0, start_val=0.0, size=(1,), method='davies_harte', device='cpu', return_numpy=False)</code>","text":"<p>Simulate Reflected Fractional Brownian Motion with barriers.</p> <p>Based on the Skorokhod reflection map applied to fBm increments.</p> <p>The process is constrained to the interval \\([lower, upper]\\) using instantaneous reflection at the boundaries. This is a continuous-path approximation of bounded diffusion.</p> Applications <ul> <li>Target zone models: Exchange rates within currency bands   (Krugman, 1991)</li> <li>Physical systems: Particles confined in a box</li> <li>Finance: Assets with hard support/resistance levels</li> <li>Queueing theory: Buffer capacities</li> </ul> Algorithm <ol> <li>Generate free fBm increments</li> <li>Apply reflection at each time step using the Skorokhod map</li> <li>Use JIT compilation for efficiency</li> </ol> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of time steps.</p> required <code>H</code> <code>float</code> <p>Hurst parameter in (0, 1).</p> required <code>lower</code> <code>float</code> <p>Lower reflection barrier.</p> <code>-1.0</code> <code>upper</code> <code>float</code> <p>Upper reflection barrier.</p> <code>1.0</code> <code>mu</code> <code>float</code> <p>Drift coefficient.</p> <code>0.0</code> <code>sigma</code> <code>float</code> <p>Volatility coefficient.</p> <code>1.0</code> <code>t_max</code> <code>float</code> <p>Total time horizon.</p> <code>1.0</code> <code>start_val</code> <code>float</code> <p>Initial value (must be in [lower, upper]).</p> <code>0.0</code> <code>size</code> <code>tuple</code> <p>Batch dimensions.</p> <code>(1,)</code> <code>method</code> <code>str</code> <p>Generation method ('davies_harte' or 'cholesky').</p> <code>'davies_harte'</code> <code>device</code> <code>str</code> <p>Computation device.</p> <code>'cpu'</code> <code>return_numpy</code> <code>bool</code> <p>If True, returns NumPy array.</p> <code>False</code> <p>Returns:</p> Type Description <p>Tensor of shape <code>(*size, n+1)</code> with paths bounded in [lower, upper].</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If n &lt;= 0.</p> Example Source code in <code>torchfbm/processes.py</code> <pre><code>def reflected_fbm(\n    n: int,\n    H: float,\n    lower: float = -1.0,\n    upper: float = 1.0,\n    mu: float = 0.0,\n    sigma: float = 1.0,\n    t_max: float = 1.0,\n    start_val: float = 0.0,\n    size: tuple = (1,),\n    method: str = \"davies_harte\",\n    device: str = \"cpu\",\n    return_numpy: bool = False,\n):\n    \"\"\"Simulate Reflected Fractional Brownian Motion with barriers.\n\n    Based on the Skorokhod reflection map applied to fBm increments.\n\n    The process is constrained to the interval $[lower, upper]$ using\n    instantaneous reflection at the boundaries. This is a continuous-path\n    approximation of bounded diffusion.\n\n    Applications:\n        - **Target zone models**: Exchange rates within currency bands\n          (Krugman, 1991)\n        - **Physical systems**: Particles confined in a box\n        - **Finance**: Assets with hard support/resistance levels\n        - **Queueing theory**: Buffer capacities\n\n    Algorithm:\n        1. Generate free fBm increments\n        2. Apply reflection at each time step using the Skorokhod map\n        3. Use JIT compilation for efficiency\n\n    Args:\n        n: Number of time steps.\n        H: Hurst parameter in (0, 1).\n        lower: Lower reflection barrier.\n        upper: Upper reflection barrier.\n        mu: Drift coefficient.\n        sigma: Volatility coefficient.\n        t_max: Total time horizon.\n        start_val: Initial value (must be in [lower, upper]).\n        size: Batch dimensions.\n        method: Generation method ('davies_harte' or 'cholesky').\n        device: Computation device.\n        return_numpy: If True, returns NumPy array.\n\n    Returns:\n        Tensor of shape ``(*size, n+1)`` with paths bounded in [lower, upper].\n\n    Raises:\n        ValueError: If n &lt;= 0.\n\n    Example:\n        &gt;&gt;&gt; # Exchange rate in a target zone\n        &gt;&gt;&gt; rate = reflected_fbm(\n        ...     n=1000, H=0.7, lower=1.0, upper=1.5, start_val=1.25\n        ... )\n    \"\"\"\n    if n &lt;= 0:\n        raise ValueError(\"n must be a positive integer\")\n\n    device = torch.device(device)\n    # 1. Generate fGn (Increments)\n    # We use fbm() to handle method/clamping, but we need the *steps*, not the path.\n    # So we call the generator directly or diff the fbm path.\n    # Let's diff the fbm path for consistency with the geometric_fbm scaling logic.\n\n    # Generate unbounded path first to get correctly scaled increments\n    unbounded_path = fbm(n, H, size, method=method, device=device, return_numpy=False)\n\n    # Scale to t_max and sigma\n    scale_factor = sigma * (t_max / n) ** H\n    unbounded_path = unbounded_path * scale_factor\n\n    # Add drift (mu * dt)\n    dt = t_max / n\n    t_grid = torch.linspace(0, t_max, n + 1, device=device).expand(*size, n + 1)\n    drift = mu * t_grid\n\n    # Combine to get the proposed increments with drift\n    total_unbounded = unbounded_path + drift\n\n    # Calculate increments (dX)\n    increments = total_unbounded[..., 1:] - total_unbounded[..., :-1]\n\n    # 2. Prepare container\n    reflected_path = torch.zeros_like(total_unbounded)\n    reflected_path[..., 0] = start_val\n\n    # 3. Run JIT Reflection\n    # Ensure bounds are floats for JIT\n    reflected_path = _apply_reflection(\n        reflected_path, increments, float(lower), float(upper)\n    )\n\n    return reflected_path.cpu().numpy() if return_numpy else reflected_path\n</code></pre>"},{"location":"api/#torchfbm.processes.reflected_fbm--exchange-rate-in-a-target-zone","title":"Exchange rate in a target zone","text":"<p>rate = reflected_fbm( ...     n=1000, H=0.7, lower=1.0, upper=1.5, start_val=1.25 ... )</p>"},{"location":"api/#transforms","title":"Transforms","text":"<p>Fractional calculus transforms (differentiation and integration).</p>"},{"location":"api/#torchfbm.transforms","title":"<code>torchfbm.transforms</code>","text":"<p>Fractional calculus transforms for time series.</p> <p>This module provides fractional differentiation and integration operators, which generalize classical calculus to non-integer orders.</p> <p>Based on Gr\u00fcnwald-Letnikov fractional derivatives, implemented via FFT for computational efficiency.</p> Example <p>from torchfbm.transforms import fractional_diff, fractional_integrate x = torch.randn(100) x_diff = fractional_diff(x, d=0.5)  # Half-derivative x_int = fractional_integrate(x, d=0.5)  # Half-integral</p>"},{"location":"api/#torchfbm.transforms.fractional_diff","title":"<code>fractional_diff(x, d, dim=-1, return_numpy=False)</code>","text":"<p>Compute the fractional derivative (or integral) of a time series.</p> <p>Based on the Gr\u00fcnwald-Letnikov definition, implemented via FFT.</p> <p>The fractional derivative of order \\(d\\) is computed in the frequency domain using the transfer function:</p> \\[H(\\omega) = (1 - e^{-i\\omega})^d\\] <p>This generalizes the standard difference operator: - \\(d = 0\\): Identity (no change) - \\(d = 1\\): First difference \\(\\Delta x_t = x_t - x_{t-1}\\) - \\(d = 0.5\\): Half-derivative (between identity and first difference) - \\(d &lt; 0\\): Fractional integration (smoothing)</p> Applications <ul> <li>Finance: Fractionally differenced series for ARFIMA models</li> <li>Memory preservation: Unlike integer differencing, fractional   differencing preserves long-range dependence while achieving   stationarity.</li> </ul> Note <p>Uses circular (periodic) boundary conditions due to FFT. Edge effects may occur at the boundaries.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor. Differentiation applied along <code>dim</code>.</p> required <code>d</code> <code>float</code> <p>Fractional order. Positive for differentiation, negative for integration.</p> required <code>dim</code> <code>int</code> <p>Dimension along which to apply the transform. Default is last dim.</p> <code>-1</code> <code>return_numpy</code> <code>bool</code> <p>If True, returns NumPy array instead of torch.Tensor.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Fractionally differentiated tensor with same shape as input.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If dimension is invalid or empty.</p> <code>TypeError</code> <p>If input dtype is not a float type.</p> Example <p>x = torch.cumsum(torch.randn(1000), dim=0)  # Random walk x_stationary = fractional_diff(x, d=0.4)  # Make stationary</p> Source code in <code>torchfbm/transforms.py</code> <pre><code>def fractional_diff(\n    x: torch.Tensor, d: float, dim: int = -1, return_numpy: bool = False\n) -&gt; torch.Tensor:\n    \"\"\"Compute the fractional derivative (or integral) of a time series.\n\n    Based on the Gr\u00fcnwald-Letnikov definition, implemented via FFT.\n\n    The fractional derivative of order $d$ is computed in the frequency domain\n    using the transfer function:\n\n    $$H(\\\\omega) = (1 - e^{-i\\\\omega})^d$$\n\n    This generalizes the standard difference operator:\n    - $d = 0$: Identity (no change)\n    - $d = 1$: First difference $\\\\Delta x_t = x_t - x_{t-1}$\n    - $d = 0.5$: Half-derivative (between identity and first difference)\n    - $d &lt; 0$: Fractional integration (smoothing)\n\n    Applications:\n        - **Finance**: Fractionally differenced series for ARFIMA models\n        - **Memory preservation**: Unlike integer differencing, fractional\n          differencing preserves long-range dependence while achieving\n          stationarity.\n\n    Note:\n        Uses circular (periodic) boundary conditions due to FFT. Edge effects\n        may occur at the boundaries.\n\n    Args:\n        x: Input tensor. Differentiation applied along `dim`.\n        d: Fractional order. Positive for differentiation, negative for integration.\n        dim: Dimension along which to apply the transform. Default is last dim.\n        return_numpy: If True, returns NumPy array instead of torch.Tensor.\n\n    Returns:\n        Fractionally differentiated tensor with same shape as input.\n\n    Raises:\n        ValueError: If dimension is invalid or empty.\n        TypeError: If input dtype is not a float type.\n\n    Example:\n        &gt;&gt;&gt; x = torch.cumsum(torch.randn(1000), dim=0)  # Random walk\n        &gt;&gt;&gt; x_stationary = fractional_diff(x, d=0.4)  # Make stationary\n        &gt;&gt;&gt; # x_stationary should have lower autocorrelation\n    \"\"\"\n    dim = dim if dim &gt;= 0 else x.dim() + dim\n    if dim &lt; 0 or dim &gt;= x.dim():\n        raise ValueError(f\"Invalid dim {dim} for input with {x.dim()} dims\")\n\n    n = x.shape[dim]\n    device = x.device\n\n    if n == 0:\n        raise ValueError(\n            \"Cannot compute fractional difference along an empty dimension\"\n        )\n\n    real_dtype = x.real.dtype if torch.is_complex(x) else x.dtype\n    if real_dtype not in (torch.float16, torch.float32, torch.float64, torch.bfloat16):\n        raise TypeError(f\"Unsupported dtype {real_dtype} for fractional_diff\")\n\n    # Frequency domain transfer function: (1 - e^(-i*omega))^d\n    freq_dtype = torch.promote_types(real_dtype, torch.float32)\n    k = torch.arange(n, device=device, dtype=freq_dtype)\n    omega = 2 * torch.tensor(torch.pi, device=device, dtype=freq_dtype) * k / n\n    transfer = (1 - torch.exp(-1j * omega)) ** d\n\n    if d &lt; 0:\n        transfer = transfer.clone()\n        transfer[0] = torch.tensor(1.0, device=device, dtype=transfer.dtype)\n\n    view_shape = [1] * x.dim()\n    view_shape[dim] = n\n    transfer = transfer.reshape(view_shape)\n\n    # Apply via FFT convolution\n    x_fft = torch.fft.fft(x, dim=dim)\n    transfer = transfer.to(device=device, dtype=x_fft.dtype)\n    diff_fft = x_fft * transfer\n    x_diff = torch.fft.ifft(diff_fft, dim=dim).real\n\n    return x_diff.cpu().numpy() if return_numpy else x_diff\n</code></pre>"},{"location":"api/#torchfbm.transforms.fractional_diff--x_stationary-should-have-lower-autocorrelation","title":"x_stationary should have lower autocorrelation","text":""},{"location":"api/#torchfbm.transforms.fractional_integrate","title":"<code>fractional_integrate(x, d, dim=-1, return_numpy=False)</code>","text":"<p>Compute the fractional integral of a time series.</p> <p>This is the inverse operation of fractional differentiation:</p> \\[I^d[x] = D^{-d}[x]\\] <p>Fractional integration \"smooths\" the series by accumulating past values with power-law decaying weights.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor. Integration applied along <code>dim</code>.</p> required <code>d</code> <code>float</code> <p>Fractional order of integration (positive values).</p> required <code>dim</code> <code>int</code> <p>Dimension along which to apply the transform.</p> <code>-1</code> <code>return_numpy</code> <code>bool</code> <p>If True, returns NumPy array instead of torch.Tensor.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Fractionally integrated tensor with same shape as input.</p> Example <p>noise = torch.randn(1000) smooth = fractional_integrate(noise, d=0.5)</p> Source code in <code>torchfbm/transforms.py</code> <pre><code>def fractional_integrate(\n    x: torch.Tensor, d: float, dim: int = -1, return_numpy: bool = False\n) -&gt; torch.Tensor:\n    \"\"\"Compute the fractional integral of a time series.\n\n    This is the inverse operation of fractional differentiation:\n\n    $$I^d[x] = D^{-d}[x]$$\n\n    Fractional integration \"smooths\" the series by accumulating past values\n    with power-law decaying weights.\n\n    Args:\n        x: Input tensor. Integration applied along `dim`.\n        d: Fractional order of integration (positive values).\n        dim: Dimension along which to apply the transform.\n        return_numpy: If True, returns NumPy array instead of torch.Tensor.\n\n    Returns:\n        Fractionally integrated tensor with same shape as input.\n\n    Example:\n        &gt;&gt;&gt; noise = torch.randn(1000)\n        &gt;&gt;&gt; smooth = fractional_integrate(noise, d=0.5)\n        &gt;&gt;&gt; # smooth has longer memory than noise\n    \"\"\"\n    return fractional_diff(x, -d, dim=dim, return_numpy=return_numpy)\n</code></pre>"},{"location":"api/#torchfbm.transforms.fractional_integrate--smooth-has-longer-memory-than-noise","title":"smooth has longer memory than noise","text":""},{"location":"api/#layers","title":"Layers","text":"<p>PyTorch neural network layers with fBm integration.</p>"},{"location":"api/#torchfbm.layers","title":"<code>torchfbm.layers</code>","text":"<p>Neural network layers with fractional Brownian motion integration.</p> <p>This module provides PyTorch layers that incorporate fractional Brownian motion and fractional Gaussian noise for exploration, regularization, and feature extraction in deep learning.</p> Layers Included <ul> <li>:class:<code>FBMNoisyLinear</code>: NoisyNet-style linear layer with fBm noise</li> <li>:class:<code>FractionalPositionalEmbedding</code>: Positional encoding using fBm paths</li> <li>:class:<code>FractionalKernel</code>: Power-law covariance kernel for attention/GPs</li> <li>:func:<code>fractional_init_</code>: Weight initialization with correlated noise</li> </ul> Example <p>from torchfbm.layers import FBMNoisyLinear, FractionalPositionalEmbedding layer = FBMNoisyLinear(64, 32, H=0.7) pos_embed = FractionalPositionalEmbedding(max_len=512, d_model=256)</p>"},{"location":"api/#torchfbm.layers.FBMNoisyLinear","title":"<code>FBMNoisyLinear</code>","text":"<p>               Bases: <code>Module</code></p> <p>Linear layer with parametric fractional Brownian motion noise.</p> <p>Based on Fortunato, Azar, Piot et al. (2017) NoisyNets, extended with fractional Gaussian noise for temporally correlated exploration.</p> <p>Standard NoisyNets use independent Gaussian noise, which provides memoryless exploration. This layer uses fGn instead, allowing: - H &gt; 0.5: Persistent exploration (smooth, trending noise) - H = 0.5: Standard NoisyNet behavior (independent noise) - H &lt; 0.5: Anti-persistent exploration (rough, oscillating noise)</p> <p>The noisy weights are computed as:</p> \\[W = \\mu_W + \\sigma_W \\odot \\epsilon_W\\] \\[b = \\mu_b + \\sigma_b \\odot \\epsilon_b\\] <p>where \\(\\epsilon_W\\) and \\(\\epsilon_b\\) are sampled from correlated fGn streams.</p> Memory Optimization <p>For large layers, full-rank noise requires \\(O(n_{out} \\times n_{in})\\) storage. The <code>rank</code> parameter enables low-rank factorization: - <code>rank='full'</code>: Independent noise per weight (expensive) - <code>rank=k</code>: Uses \\(k\\) rank-1 outer products (efficient)</p> <p>Parameters:</p> Name Type Description Default <code>in_features</code> <code>int</code> <p>Number of input features.</p> required <code>out_features</code> <code>int</code> <p>Number of output features.</p> required <code>H</code> <code>float</code> <p>Hurst parameter for fBm noise (0 &lt; H &lt; 1).</p> <code>0.5</code> <code>sigma_init</code> <code>float</code> <p>Initial scale for learnable noise parameters.</p> <code>0.5</code> <code>rank</code> <code>Union[int, str]</code> <p>Noise rank ('full' for independent, or integer for low-rank).</p> <code>1</code> <code>buffer_size</code> <code>int</code> <p>Number of pre-generated noise samples.</p> <code>1000</code> <code>method</code> <code>str</code> <p>Generation method ('davies_harte' or 'cholesky').</p> <code>'davies_harte'</code> <code>device</code> <code>str</code> <p>Device to store parameters and buffers.</p> <code>'cpu'</code> <code>dtype</code> <code>dtype</code> <p>Data type for parameters and buffers.</p> <code>float32</code> <code>seed</code> <code>Optional[int]</code> <p>Random seed for reproducibility.</p> <code>None</code> Example Source code in <code>torchfbm/layers.py</code> <pre><code>class FBMNoisyLinear(nn.Module):\n    \"\"\"Linear layer with parametric fractional Brownian motion noise.\n\n    Based on Fortunato, Azar, Piot et al. (2017) NoisyNets, extended with\n    fractional Gaussian noise for temporally correlated exploration.\n\n    Standard NoisyNets use independent Gaussian noise, which provides\n    memoryless exploration. This layer uses fGn instead, allowing:\n    - **H &gt; 0.5**: Persistent exploration (smooth, trending noise)\n    - **H = 0.5**: Standard NoisyNet behavior (independent noise)\n    - **H &lt; 0.5**: Anti-persistent exploration (rough, oscillating noise)\n\n    The noisy weights are computed as:\n\n    $$W = \\\\mu_W + \\\\sigma_W \\\\odot \\\\epsilon_W$$\n\n    $$b = \\\\mu_b + \\\\sigma_b \\\\odot \\\\epsilon_b$$\n\n    where $\\\\epsilon_W$ and $\\\\epsilon_b$ are sampled from correlated\n    fGn streams.\n\n    Memory Optimization:\n        For large layers, full-rank noise requires $O(n_{out} \\\\times n_{in})$\n        storage. The ``rank`` parameter enables low-rank factorization:\n        - ``rank='full'``: Independent noise per weight (expensive)\n        - ``rank=k``: Uses $k$ rank-1 outer products (efficient)\n\n    Args:\n        in_features: Number of input features.\n        out_features: Number of output features.\n        H: Hurst parameter for fBm noise (0 &lt; H &lt; 1).\n        sigma_init: Initial scale for learnable noise parameters.\n        rank: Noise rank ('full' for independent, or integer for low-rank).\n        buffer_size: Number of pre-generated noise samples.\n        method: Generation method ('davies_harte' or 'cholesky').\n        device: Device to store parameters and buffers.\n        dtype: Data type for parameters and buffers.\n        seed: Random seed for reproducibility.\n\n    Example:\n        &gt;&gt;&gt; # Replace standard linear in DQN for exploration\n        &gt;&gt;&gt; layer = FBMNoisyLinear(64, 32, H=0.7, rank=4)\n        &gt;&gt;&gt; layer.train()  # Noisy weights during training\n        &gt;&gt;&gt; output = layer(input)\n        &gt;&gt;&gt; layer.eval()   # Mean weights during inference\n        &gt;&gt;&gt; output = layer(input)\n    \"\"\"\n\n    def __init__(\n        self,\n        in_features: int,\n        out_features: int,\n        H: float = 0.5,\n        sigma_init: float = 0.5,\n        rank: Union[int, str] = 1,  # 1, 10, or \"full\"\n        buffer_size: int = 1000,\n        method: str = \"davies_harte\",\n        device: str = \"cpu\",\n        dtype: torch.dtype = torch.float32,\n        seed: Optional[int] = None,\n    ):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.H = H\n        self.rank = rank\n        self.buffer_size = buffer_size\n        self.method = method\n        self.step_counter = 0\n        self._device = torch.device(device)\n        self._dtype = dtype\n        self._seed = seed\n\n        # --- Learnable Parameters ---\n        self.weight_mu = nn.Parameter(\n            torch.empty(out_features, in_features, device=self._device, dtype=self._dtype)\n        )\n        self.weight_sigma = nn.Parameter(\n            torch.empty(out_features, in_features, device=self._device, dtype=self._dtype)\n        )\n        self.bias_mu = nn.Parameter(\n            torch.empty(out_features, device=self._device, dtype=self._dtype)\n        )\n        self.bias_sigma = nn.Parameter(\n            torch.empty(out_features, device=self._device, dtype=self._dtype)\n        )\n\n        # --- Output Buffers (for sampled noise) ---\n        self.register_buffer(\n            \"weight_epsilon\", \n            torch.empty(out_features, in_features, device=self._device, dtype=self._dtype)\n        )\n        self.register_buffer(\n            \"bias_epsilon\", \n            torch.empty(out_features, device=self._device, dtype=self._dtype)\n        )\n\n        # --- Stream Buffers (The reservoir of random numbers) ---\n        # 1. Bias is ALWAYS independent (Full Rank) because it's cheap (O(N))\n        self.register_buffer(\n            \"noise_bias\", \n            torch.empty(out_features, buffer_size, device=self._device, dtype=self._dtype)\n        )\n\n        # 2. Weight Noise Allocation\n        if self.rank == \"full\":\n            # Independent: O(N_out * N_in) - Expensive\n            self.register_buffer(\n                \"noise_weight_source\", \n                torch.empty(out_features, in_features, buffer_size, device=self._device, dtype=self._dtype)\n            )\n            if (in_features * out_features) &gt; 1e9:\n                print(\n                    f\"Warning: Using full-rank noise for weights with size \"\n                    f\"({out_features}, {in_features}) may consume significant memory. Consider setting rank to a smaller integer.\"\n                )\n        else:\n            # Low-Rank: O(Rank * (N_out + N_in)) memory - Efficient\n            r = int(self.rank)\n            self.register_buffer(\n                \"noise_in\", \n                torch.empty(r, in_features, buffer_size, device=self._device, dtype=self._dtype)\n            )\n            self.register_buffer(\n                \"noise_out\", \n                torch.empty(r, out_features, buffer_size, device=self._device, dtype=self._dtype)\n            )\n\n        self.reset_parameters(sigma_init)\n        self.refresh_noise_stream()\n\n    def reset_parameters(self, sigma_init):\n        \"\"\"Initialize layer parameters.\n\n        Uses uniform initialization for mean parameters and constant\n        initialization for sigma parameters.\n\n        Args:\n            sigma_init: Initial value for noise scaling parameters.\n        \"\"\"\n        mu_range = 1 / self.in_features**0.5\n        self.weight_mu.data.uniform_(-mu_range, mu_range)\n        self.weight_sigma.data.fill_(sigma_init / self.in_features**0.5)\n        self.bias_mu.data.uniform_(-mu_range, mu_range)\n        self.bias_sigma.data.fill_(sigma_init / self.out_features**0.5)\n\n    def refresh_noise_stream(self):\n        \"\"\"Regenerate the fGn noise buffers.\n\n        Called automatically when the buffer is exhausted during forward passes.\n        Uses different seeds for weight and bias noise to ensure independence.\n        \"\"\"\n        gen_func = generate_cholesky if self.method == \"cholesky\" else generate_davies_harte\n\n        # --- Seed Management ---\n        # We ensure distinct seeds for every component to prevent correlation\n        s_bias = self._seed\n        s_in = self._seed + 1 if self._seed is not None else None\n        s_out = self._seed + 2 if self._seed is not None else None\n        s_full = self._seed + 3 if self._seed is not None else None\n\n        # 1. Generate Bias Noise (Always Independent)\n        self.noise_bias = gen_func(\n            self.buffer_size, self.H, (self.out_features,), \n            self._device, self._dtype, seed=s_bias\n        )\n\n        # 2. Generate Weight Noise\n        if self.rank == \"full\":\n            # Flatten, generate, reshape\n            total = self.out_features * self.in_features\n            raw = gen_func(\n                self.buffer_size, self.H, (total,), \n                self._device, self._dtype, seed=s_full\n            )\n            self.noise_weight_source = raw.view(self.out_features, self.in_features, self.buffer_size)\n\n        else:\n            r = int(self.rank)\n\n            # Input Factors (Rank, In)\n            noise_in_flat = gen_func(\n                self.buffer_size, self.H, (r * self.in_features,), \n                self._device, self._dtype, seed=s_in\n            )\n            self.noise_in = noise_in_flat.view(r, self.in_features, self.buffer_size)\n\n            # Output Factors (Rank, Out)\n            noise_out_flat = gen_func(\n                self.buffer_size, self.H, (r * self.out_features,), \n                self._device, self._dtype, seed=s_out\n            )\n            self.noise_out = noise_out_flat.view(r, self.out_features, self.buffer_size)\n\n        self.step_counter = 0\n\n    def sample_noise(self):\n        \"\"\"Sample noise for current forward pass.\n\n        Reads from pre-generated buffers and constructs weight/bias noise.\n        For low-rank mode, synthesizes full noise matrix from rank-k factors.\n        \"\"\"\n        if self.step_counter &gt;= self.buffer_size:\n            self.refresh_noise_stream()\n\n        # Bias is always simple lookup\n        self.bias_epsilon = self.noise_bias[..., self.step_counter]\n\n        if self.rank == \"full\":\n            self.weight_epsilon = self.noise_weight_source[..., self.step_counter]\n\n        else:\n            # Low-Rank Synthesis\n            u = self.noise_in[..., self.step_counter]   # (Rank, In)\n            v = self.noise_out[..., self.step_counter]  # (Rank, Out)\n\n            # Apply Factorized NoisyNet transform: f(x) = sign(x) * sqrt(abs(x))\n            # This preserves the magnitude distribution when multiplying two Gaussians\n            def f(x): return x.sign().mul(x.abs().sqrt())\n            u_hat = f(u)\n            v_hat = f(v)\n\n            # Sum of Outer Products: W = (1/sqrt(K)) * Sum(v_k (x) u_k)\n            # einsum: rank(r), out(o), in(i) -&gt; out(o), in(i)\n            matrix_noise = torch.einsum('ro, ri -&gt; oi', v_hat, u_hat)\n\n            # Scale to maintain unit variance\n            scale = 1.0 / (int(self.rank) ** 0.5)\n            self.weight_epsilon = matrix_noise * scale\n\n        self.step_counter += 1\n\n    def forward(self, input):\n        \"\"\"Forward pass with optional noise injection.\n\n        During training: Uses noisy weights $\\\\mu + \\\\sigma \\\\odot \\\\epsilon$\n        During eval: Uses only mean weights $\\\\mu$\n\n        Args:\n            input: Input tensor of shape ``(batch, in_features)``.\n\n        Returns:\n            Output tensor of shape ``(batch, out_features)``.\n        \"\"\"\n        if self.training:\n            self.sample_noise()\n            return F.linear(\n                input,\n                self.weight_mu + self.weight_sigma * self.weight_epsilon,\n                self.bias_mu + self.bias_sigma * self.bias_epsilon,\n            )\n        return F.linear(input, self.weight_mu, self.bias_mu)\n</code></pre>"},{"location":"api/#torchfbm.layers.FBMNoisyLinear--replace-standard-linear-in-dqn-for-exploration","title":"Replace standard linear in DQN for exploration","text":"<p>layer = FBMNoisyLinear(64, 32, H=0.7, rank=4) layer.train()  # Noisy weights during training output = layer(input) layer.eval()   # Mean weights during inference output = layer(input)</p>"},{"location":"api/#torchfbm.layers.FBMNoisyLinear.forward","title":"<code>forward(input)</code>","text":"<p>Forward pass with optional noise injection.</p> <p>During training: Uses noisy weights \\(\\mu + \\sigma \\odot \\epsilon\\) During eval: Uses only mean weights \\(\\mu\\)</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <p>Input tensor of shape <code>(batch, in_features)</code>.</p> required <p>Returns:</p> Type Description <p>Output tensor of shape <code>(batch, out_features)</code>.</p> Source code in <code>torchfbm/layers.py</code> <pre><code>def forward(self, input):\n    \"\"\"Forward pass with optional noise injection.\n\n    During training: Uses noisy weights $\\\\mu + \\\\sigma \\\\odot \\\\epsilon$\n    During eval: Uses only mean weights $\\\\mu$\n\n    Args:\n        input: Input tensor of shape ``(batch, in_features)``.\n\n    Returns:\n        Output tensor of shape ``(batch, out_features)``.\n    \"\"\"\n    if self.training:\n        self.sample_noise()\n        return F.linear(\n            input,\n            self.weight_mu + self.weight_sigma * self.weight_epsilon,\n            self.bias_mu + self.bias_sigma * self.bias_epsilon,\n        )\n    return F.linear(input, self.weight_mu, self.bias_mu)\n</code></pre>"},{"location":"api/#torchfbm.layers.FBMNoisyLinear.refresh_noise_stream","title":"<code>refresh_noise_stream()</code>","text":"<p>Regenerate the fGn noise buffers.</p> <p>Called automatically when the buffer is exhausted during forward passes. Uses different seeds for weight and bias noise to ensure independence.</p> Source code in <code>torchfbm/layers.py</code> <pre><code>def refresh_noise_stream(self):\n    \"\"\"Regenerate the fGn noise buffers.\n\n    Called automatically when the buffer is exhausted during forward passes.\n    Uses different seeds for weight and bias noise to ensure independence.\n    \"\"\"\n    gen_func = generate_cholesky if self.method == \"cholesky\" else generate_davies_harte\n\n    # --- Seed Management ---\n    # We ensure distinct seeds for every component to prevent correlation\n    s_bias = self._seed\n    s_in = self._seed + 1 if self._seed is not None else None\n    s_out = self._seed + 2 if self._seed is not None else None\n    s_full = self._seed + 3 if self._seed is not None else None\n\n    # 1. Generate Bias Noise (Always Independent)\n    self.noise_bias = gen_func(\n        self.buffer_size, self.H, (self.out_features,), \n        self._device, self._dtype, seed=s_bias\n    )\n\n    # 2. Generate Weight Noise\n    if self.rank == \"full\":\n        # Flatten, generate, reshape\n        total = self.out_features * self.in_features\n        raw = gen_func(\n            self.buffer_size, self.H, (total,), \n            self._device, self._dtype, seed=s_full\n        )\n        self.noise_weight_source = raw.view(self.out_features, self.in_features, self.buffer_size)\n\n    else:\n        r = int(self.rank)\n\n        # Input Factors (Rank, In)\n        noise_in_flat = gen_func(\n            self.buffer_size, self.H, (r * self.in_features,), \n            self._device, self._dtype, seed=s_in\n        )\n        self.noise_in = noise_in_flat.view(r, self.in_features, self.buffer_size)\n\n        # Output Factors (Rank, Out)\n        noise_out_flat = gen_func(\n            self.buffer_size, self.H, (r * self.out_features,), \n            self._device, self._dtype, seed=s_out\n        )\n        self.noise_out = noise_out_flat.view(r, self.out_features, self.buffer_size)\n\n    self.step_counter = 0\n</code></pre>"},{"location":"api/#torchfbm.layers.FBMNoisyLinear.reset_parameters","title":"<code>reset_parameters(sigma_init)</code>","text":"<p>Initialize layer parameters.</p> <p>Uses uniform initialization for mean parameters and constant initialization for sigma parameters.</p> <p>Parameters:</p> Name Type Description Default <code>sigma_init</code> <p>Initial value for noise scaling parameters.</p> required Source code in <code>torchfbm/layers.py</code> <pre><code>def reset_parameters(self, sigma_init):\n    \"\"\"Initialize layer parameters.\n\n    Uses uniform initialization for mean parameters and constant\n    initialization for sigma parameters.\n\n    Args:\n        sigma_init: Initial value for noise scaling parameters.\n    \"\"\"\n    mu_range = 1 / self.in_features**0.5\n    self.weight_mu.data.uniform_(-mu_range, mu_range)\n    self.weight_sigma.data.fill_(sigma_init / self.in_features**0.5)\n    self.bias_mu.data.uniform_(-mu_range, mu_range)\n    self.bias_sigma.data.fill_(sigma_init / self.out_features**0.5)\n</code></pre>"},{"location":"api/#torchfbm.layers.FBMNoisyLinear.sample_noise","title":"<code>sample_noise()</code>","text":"<p>Sample noise for current forward pass.</p> <p>Reads from pre-generated buffers and constructs weight/bias noise. For low-rank mode, synthesizes full noise matrix from rank-k factors.</p> Source code in <code>torchfbm/layers.py</code> <pre><code>def sample_noise(self):\n    \"\"\"Sample noise for current forward pass.\n\n    Reads from pre-generated buffers and constructs weight/bias noise.\n    For low-rank mode, synthesizes full noise matrix from rank-k factors.\n    \"\"\"\n    if self.step_counter &gt;= self.buffer_size:\n        self.refresh_noise_stream()\n\n    # Bias is always simple lookup\n    self.bias_epsilon = self.noise_bias[..., self.step_counter]\n\n    if self.rank == \"full\":\n        self.weight_epsilon = self.noise_weight_source[..., self.step_counter]\n\n    else:\n        # Low-Rank Synthesis\n        u = self.noise_in[..., self.step_counter]   # (Rank, In)\n        v = self.noise_out[..., self.step_counter]  # (Rank, Out)\n\n        # Apply Factorized NoisyNet transform: f(x) = sign(x) * sqrt(abs(x))\n        # This preserves the magnitude distribution when multiplying two Gaussians\n        def f(x): return x.sign().mul(x.abs().sqrt())\n        u_hat = f(u)\n        v_hat = f(v)\n\n        # Sum of Outer Products: W = (1/sqrt(K)) * Sum(v_k (x) u_k)\n        # einsum: rank(r), out(o), in(i) -&gt; out(o), in(i)\n        matrix_noise = torch.einsum('ro, ri -&gt; oi', v_hat, u_hat)\n\n        # Scale to maintain unit variance\n        scale = 1.0 / (int(self.rank) ** 0.5)\n        self.weight_epsilon = matrix_noise * scale\n\n    self.step_counter += 1\n</code></pre>"},{"location":"api/#torchfbm.layers.FractionalKernel","title":"<code>FractionalKernel</code>","text":"<p>               Bases: <code>Module</code></p> <p>Power-law covariance kernel based on fractional distance.</p> <p>Computes similarity based on the fractional Brownian motion covariance structure, where correlation decays as a power law with distance.</p> <p>The kernel is defined as:</p> \\[K(x, y) = \\exp\\left(-\\left(\\frac{\\|x - y\\|}{\\ell}\\right)^{2H}\\right)\\] <p>where: - \\(\\|x - y\\|\\) is the Euclidean distance - \\(\\ell\\) is the length scale - \\(H\\) is the Hurst parameter</p> Properties <ul> <li>H = 0.5: Standard squared exponential (RBF) kernel</li> <li>H &lt; 0.5: Rougher kernel (faster decay)</li> <li>H &gt; 0.5: Smoother kernel (slower decay)</li> </ul> Applications <ul> <li>Attention mechanisms: Fractal-aware attention patterns</li> <li>Gaussian Processes: Long-memory covariance functions</li> <li>Kernel methods: SVMs with power-law similarity</li> </ul> <p>Parameters:</p> Name Type Description Default <code>H</code> <code>float</code> <p>Hurst parameter controlling decay rate.</p> <code>0.5</code> <code>length_scale</code> <code>float</code> <p>Characteristic length scale \\(\\ell\\).</p> <code>1.0</code> Example <p>kernel = FractionalKernel(H=0.7, length_scale=1.0) x1 = torch.randn(32, 10, 64)  # (batch, n_points, dim) x2 = torch.randn(32, 20, 64)  # (batch, m_points, dim) K = kernel(x1, x2)  # (32, 10, 20) similarity matrix</p> Source code in <code>torchfbm/layers.py</code> <pre><code>class FractionalKernel(nn.Module):\n    \"\"\"Power-law covariance kernel based on fractional distance.\n\n    Computes similarity based on the fractional Brownian motion covariance\n    structure, where correlation decays as a power law with distance.\n\n    The kernel is defined as:\n\n    $$K(x, y) = \\\\exp\\\\left(-\\\\left(\\\\frac{\\\\|x - y\\\\|}{\\\\ell}\\\\right)^{2H}\\\\right)$$\n\n    where:\n    - $\\\\|x - y\\\\|$ is the Euclidean distance\n    - $\\\\ell$ is the length scale\n    - $H$ is the Hurst parameter\n\n    Properties:\n        - **H = 0.5**: Standard squared exponential (RBF) kernel\n        - **H &lt; 0.5**: Rougher kernel (faster decay)\n        - **H &gt; 0.5**: Smoother kernel (slower decay)\n\n    Applications:\n        - **Attention mechanisms**: Fractal-aware attention patterns\n        - **Gaussian Processes**: Long-memory covariance functions\n        - **Kernel methods**: SVMs with power-law similarity\n\n    Args:\n        H: Hurst parameter controlling decay rate.\n        length_scale: Characteristic length scale $\\\\ell$.\n\n    Example:\n        &gt;&gt;&gt; kernel = FractionalKernel(H=0.7, length_scale=1.0)\n        &gt;&gt;&gt; x1 = torch.randn(32, 10, 64)  # (batch, n_points, dim)\n        &gt;&gt;&gt; x2 = torch.randn(32, 20, 64)  # (batch, m_points, dim)\n        &gt;&gt;&gt; K = kernel(x1, x2)  # (32, 10, 20) similarity matrix\n    \"\"\"\n\n    def __init__(self, H: float = 0.5, length_scale: float = 1.0):\n        super().__init__()\n        self.H = H\n        self.length_scale = length_scale\n\n    def forward(self, x1, x2):\n        \"\"\"Compute pairwise kernel values.\n\n        Args:\n            x1: First set of points, shape ``(batch, n, dim)``.\n            x2: Second set of points, shape ``(batch, m, dim)``.\n\n        Returns:\n            Kernel matrix of shape ``(batch, n, m)``.\n        \"\"\"\n        # x1: (B, N, D)\n        # x2: (B, M, D)\n        # Compute pairwise distances\n        dist = torch.cdist(x1, x2, p=2)  # Euclidean dist\n\n        # Fractional similarity\n        # We invert it so closer = higher similarity\n        # Kernel = exp( - (dist / length_scale)^(2H) )\n        return torch.exp(-torch.pow(dist / self.length_scale, 2 * self.H))\n</code></pre>"},{"location":"api/#torchfbm.layers.FractionalKernel.forward","title":"<code>forward(x1, x2)</code>","text":"<p>Compute pairwise kernel values.</p> <p>Parameters:</p> Name Type Description Default <code>x1</code> <p>First set of points, shape <code>(batch, n, dim)</code>.</p> required <code>x2</code> <p>Second set of points, shape <code>(batch, m, dim)</code>.</p> required <p>Returns:</p> Type Description <p>Kernel matrix of shape <code>(batch, n, m)</code>.</p> Source code in <code>torchfbm/layers.py</code> <pre><code>def forward(self, x1, x2):\n    \"\"\"Compute pairwise kernel values.\n\n    Args:\n        x1: First set of points, shape ``(batch, n, dim)``.\n        x2: Second set of points, shape ``(batch, m, dim)``.\n\n    Returns:\n        Kernel matrix of shape ``(batch, n, m)``.\n    \"\"\"\n    # x1: (B, N, D)\n    # x2: (B, M, D)\n    # Compute pairwise distances\n    dist = torch.cdist(x1, x2, p=2)  # Euclidean dist\n\n    # Fractional similarity\n    # We invert it so closer = higher similarity\n    # Kernel = exp( - (dist / length_scale)^(2H) )\n    return torch.exp(-torch.pow(dist / self.length_scale, 2 * self.H))\n</code></pre>"},{"location":"api/#torchfbm.layers.FractionalPositionalEmbedding","title":"<code>FractionalPositionalEmbedding</code>","text":"<p>               Bases: <code>Module</code></p> <p>Positional embedding using frozen fractional Brownian motion paths.</p> <p>Uses diverse fBm paths with varying Hurst parameters to create positional encodings that capture multi-scale fractal structure.</p> <p>Unlike sinusoidal embeddings which encode position at fixed frequencies, fBm embeddings encode position at varying levels of roughness: - Low H channels: High-frequency, local position information - High H channels: Low-frequency, global position trends</p> <p>The embedding is computed once during initialization and frozen (non-learnable), similar to standard positional encodings.</p> Algorithm <ol> <li>Generate <code>d_model</code> fBm paths with Hurst parameters    linearly spaced in <code>H_range</code></li> <li>Normalize each path to zero mean and unit variance</li> <li>Store as frozen buffer</li> </ol> <p>Parameters:</p> Name Type Description Default <code>max_len</code> <p>Maximum sequence length supported.</p> required <code>d_model</code> <p>Embedding dimension (number of fBm paths).</p> required <code>H_range</code> <p>Tuple of (H_min, H_max) for diverse roughness levels.</p> <code>(0.1, 0.9)</code> <code>method</code> <p>Generation method ('davies_harte' or 'cholesky').</p> <code>'davies_harte'</code> <code>device</code> <p>Device to store embeddings.</p> <code>'cpu'</code> <code>dtype</code> <p>Data type for embeddings.</p> <code>float32</code> <code>seed</code> <code>Optional[int]</code> <p>Random seed for reproducibility.</p> <code>None</code> Example <p>embed = FractionalPositionalEmbedding( ...     max_len=512, d_model=256, H_range=(0.1, 0.9) ... ) x = torch.randn(32, 100, 256)  # (batch, seq, dim) x_with_pos = embed(x)  # Adds positional encoding</p> Source code in <code>torchfbm/layers.py</code> <pre><code>class FractionalPositionalEmbedding(nn.Module):\n    \"\"\"Positional embedding using frozen fractional Brownian motion paths.\n\n    Uses diverse fBm paths with varying Hurst parameters to create\n    positional encodings that capture multi-scale fractal structure.\n\n    Unlike sinusoidal embeddings which encode position at fixed frequencies,\n    fBm embeddings encode position at varying levels of roughness:\n    - Low H channels: High-frequency, local position information\n    - High H channels: Low-frequency, global position trends\n\n    The embedding is computed once during initialization and frozen\n    (non-learnable), similar to standard positional encodings.\n\n    Algorithm:\n        1. Generate ``d_model`` fBm paths with Hurst parameters\n           linearly spaced in ``H_range``\n        2. Normalize each path to zero mean and unit variance\n        3. Store as frozen buffer\n\n    Args:\n        max_len: Maximum sequence length supported.\n        d_model: Embedding dimension (number of fBm paths).\n        H_range: Tuple of (H_min, H_max) for diverse roughness levels.\n        method: Generation method ('davies_harte' or 'cholesky').\n        device: Device to store embeddings.\n        dtype: Data type for embeddings.\n        seed: Random seed for reproducibility.\n\n    Example:\n        &gt;&gt;&gt; embed = FractionalPositionalEmbedding(\n        ...     max_len=512, d_model=256, H_range=(0.1, 0.9)\n        ... )\n        &gt;&gt;&gt; x = torch.randn(32, 100, 256)  # (batch, seq, dim)\n        &gt;&gt;&gt; x_with_pos = embed(x)  # Adds positional encoding\n    \"\"\"\n\n    def __init__(\n        self,\n        max_len,\n        d_model,\n        H_range=(0.1, 0.9),\n        method=\"davies_harte\",\n        device=\"cpu\",\n        dtype=torch.float32,\n        seed: Optional[int] = None,\n    ):\n        super().__init__()\n        self.d_model = d_model\n        self.method = method\n        self._device = torch.device(device)\n        self._dtype = dtype\n        self._seed = seed\n\n        # 1. Create H values from H_range[0] to H_range[1]\n        # We ensure they are within safe bounds [0.01, 0.99]\n        h_min = max(0.01, H_range[0])\n        h_max = min(0.99, H_range[1])\n        Hs = torch.linspace(\n            h_min, h_max, d_model, device=self._device, dtype=self._dtype\n        )\n\n        embeddings = []\n        for i in range(d_model):\n            # Generate path using selected method\n            # We generate on CPU initially to save GPU memory for weights,\n            # then register as buffer moves it to device automatically.\n            path = fbm(\n                max_len,\n                H=Hs[i].item(),\n                size=(1,),\n                method=self.method,\n                device=self._device,\n                dtype=self._dtype,\n                seed=self._seed,\n            ).squeeze()\n\n            # Normalization (Critical for Embeddings to preserve gradient scale)\n            path = (path - path.mean()) / (path.std() + 1e-6)\n            embeddings.append(path)\n\n        # Shape: (max_len, d_model)\n        # We assume the fbm path length (max_len+1) needs to be trimmed or matched\n        # fbm() returns n+1 points, take 1 to max_len+1\n        pe_tensor = torch.stack(embeddings, dim=1)\n\n        # Crop if fbm generated n+1 and we want max_len\n        pe_tensor = pe_tensor[:max_len, :]\n\n        self.register_buffer(\"pe\", pe_tensor.to(self._device, self._dtype))\n\n    def forward(self, x):\n        \"\"\"Add positional encoding to input.\n\n        Args:\n            x: Input tensor of shape ``(batch, seq_len, d_model)``.\n\n        Returns:\n            Tensor with positional encoding added, same shape as input.\n\n        Raises:\n            ValueError: If sequence length exceeds ``max_len``.\n        \"\"\"\n        seq_len = x.size(1)\n        if seq_len &gt; self.pe.size(0):\n            raise ValueError(\n                f\"Sequence length {seq_len} exceeds max_len {self.pe.size(0)}\"\n            )\n\n        return x + self.pe[:seq_len, :].unsqueeze(0)\n</code></pre>"},{"location":"api/#torchfbm.layers.FractionalPositionalEmbedding.forward","title":"<code>forward(x)</code>","text":"<p>Add positional encoding to input.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <p>Input tensor of shape <code>(batch, seq_len, d_model)</code>.</p> required <p>Returns:</p> Type Description <p>Tensor with positional encoding added, same shape as input.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If sequence length exceeds <code>max_len</code>.</p> Source code in <code>torchfbm/layers.py</code> <pre><code>def forward(self, x):\n    \"\"\"Add positional encoding to input.\n\n    Args:\n        x: Input tensor of shape ``(batch, seq_len, d_model)``.\n\n    Returns:\n        Tensor with positional encoding added, same shape as input.\n\n    Raises:\n        ValueError: If sequence length exceeds ``max_len``.\n    \"\"\"\n    seq_len = x.size(1)\n    if seq_len &gt; self.pe.size(0):\n        raise ValueError(\n            f\"Sequence length {seq_len} exceeds max_len {self.pe.size(0)}\"\n        )\n\n    return x + self.pe[:seq_len, :].unsqueeze(0)\n</code></pre>"},{"location":"api/#torchfbm.layers.fractional_init_","title":"<code>fractional_init_(tensor, H=0.7, std=0.02)</code>","text":"<p>Initialize tensor weights using fractional Gaussian noise (in-place).</p> <p>Fills the tensor with correlated fGn values, inducing long-range spatial correlations in the weight matrix. May be useful for: - CNN kernels where adjacent weights should be correlated - Inducing smoothness priors in weight space - Experimental architectures with structured initialization</p> <p>The fGn is normalized to zero mean and scaled to the target standard deviation.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>Tensor to initialize (modified in-place).</p> required <code>H</code> <code>float</code> <p>Hurst parameter controlling spatial correlation.</p> <code>0.7</code> <code>std</code> <code>float</code> <p>Target standard deviation of initialized weights.</p> <code>0.02</code> Example <p>linear = nn.Linear(64, 32) fractional_init_(linear.weight, H=0.7, std=0.02)</p> Source code in <code>torchfbm/layers.py</code> <pre><code>def fractional_init_(tensor: torch.Tensor, H: float = 0.7, std: float = 0.02):\n    \"\"\"Initialize tensor weights using fractional Gaussian noise (in-place).\n\n    Fills the tensor with correlated fGn values, inducing long-range\n    spatial correlations in the weight matrix. May be useful for:\n    - CNN kernels where adjacent weights should be correlated\n    - Inducing smoothness priors in weight space\n    - Experimental architectures with structured initialization\n\n    The fGn is normalized to zero mean and scaled to the target standard\n    deviation.\n\n    Args:\n        tensor: Tensor to initialize (modified in-place).\n        H: Hurst parameter controlling spatial correlation.\n        std: Target standard deviation of initialized weights.\n\n    Example:\n        &gt;&gt;&gt; linear = nn.Linear(64, 32)\n        &gt;&gt;&gt; fractional_init_(linear.weight, H=0.7, std=0.02)\n    \"\"\"\n    with torch.no_grad():\n        rows, cols = tensor.shape\n        total_elements = rows * cols\n\n        # Generate a long correlated stream\n        fgn = generate_davies_harte(total_elements, H, device=tensor.device)\n\n        # Normalize and Scale\n        fgn = (fgn - fgn.mean()) / (fgn.std() + 1e-8)\n        fgn = fgn * std\n\n        tensor.copy_(fgn.view(rows, cols))\n</code></pre>"},{"location":"api/#sde","title":"SDE","text":"<p>Fractional stochastic differential equation solvers.</p>"},{"location":"api/#torchfbm.sde","title":"<code>torchfbm.sde</code>","text":"<p>Fractional stochastic differential equation solvers.</p> <p>This module provides neural network modules for solving stochastic differential equations driven by fractional Brownian motion.</p> <p>Based on the theory of rough paths and fractional calculus for SDEs.</p> Example <p>from torchfbm.sde import NeuralFSDE drift = nn.Linear(2, 2) diffusion = nn.Linear(2, 2) fsde = NeuralFSDE(state_size=2, drift_net=drift, diffusion_net=diffusion) x0 = torch.randn(32, 2) trajectory = fsde(x0, n_steps=100)</p>"},{"location":"api/#torchfbm.sde.NeuralFSDE","title":"<code>NeuralFSDE</code>","text":"<p>               Bases: <code>Module</code></p> <p>Neural network solver for fractional stochastic differential equations.</p> <p>Solves SDEs of the form:</p> \\[dX_t = \\mu(X_t, t)dt + \\sigma(X_t, t)dB^H_t\\] <p>where: - \\(\\mu\\) is the drift network - \\(\\sigma\\) is the diffusion network - \\(B^H_t\\) is fractional Brownian motion with Hurst parameter \\(H\\)</p> <p>Uses Euler-Maruyama discretization with fGn increments. The Hurst parameter can optionally be learned during training.</p> Note <p>For \\(H &lt; 0.5\\) (rough regime), standard Euler-Maruyama may be unstable. This implementation raises an error for \\(H &lt; 0.5\\) until rough path integration methods are implemented.</p> Algorithm <p>For each step \\(i\\):</p> \\[X_{i+1} = X_i + \\mu(X_i) \\Delta t + \\sigma(X_i) \\cdot (\\Delta t)^H \\epsilon_i\\] <p>where \\(\\epsilon_i\\) is fGn with the specified Hurst parameter.</p> <p>Parameters:</p> Name Type Description Default <code>state_size</code> <p>Dimension of the state space.</p> required <code>drift_net</code> <p>Neural network for drift \\(\\mu(X_t)\\). Input/output: <code>state_size</code>.</p> required <code>diffusion_net</code> <p>Neural network for diffusion \\(\\sigma(X_t)\\). Input/output: <code>state_size</code>.</p> required <code>H_init</code> <p>Initial Hurst parameter value.</p> <code>0.5</code> <code>learnable_H</code> <p>If True, H is a learnable parameter.</p> <code>False</code> <code>t_max</code> <p>Total time horizon for integration.</p> <code>1.0</code> Example Source code in <code>torchfbm/sde.py</code> <pre><code>class NeuralFSDE(nn.Module):\n    \"\"\"Neural network solver for fractional stochastic differential equations.\n\n    Solves SDEs of the form:\n\n    $$dX_t = \\\\mu(X_t, t)dt + \\\\sigma(X_t, t)dB^H_t$$\n\n    where:\n    - $\\\\mu$ is the drift network\n    - $\\\\sigma$ is the diffusion network\n    - $B^H_t$ is fractional Brownian motion with Hurst parameter $H$\n\n    Uses Euler-Maruyama discretization with fGn increments. The Hurst\n    parameter can optionally be learned during training.\n\n    Note:\n        For $H &lt; 0.5$ (rough regime), standard Euler-Maruyama may be unstable.\n        This implementation raises an error for $H &lt; 0.5$ until rough path\n        integration methods are implemented.\n\n    Algorithm:\n        For each step $i$:\n\n        $$X_{i+1} = X_i + \\\\mu(X_i) \\\\Delta t + \\\\sigma(X_i) \\\\cdot (\\\\Delta t)^H \\\\epsilon_i$$\n\n        where $\\\\epsilon_i$ is fGn with the specified Hurst parameter.\n\n    Args:\n        state_size: Dimension of the state space.\n        drift_net: Neural network for drift $\\\\mu(X_t)$. Input/output: ``state_size``.\n        diffusion_net: Neural network for diffusion $\\\\sigma(X_t)$. Input/output: ``state_size``.\n        H_init: Initial Hurst parameter value.\n        learnable_H: If True, H is a learnable parameter.\n        t_max: Total time horizon for integration.\n\n    Example:\n        &gt;&gt;&gt; # Define drift and diffusion networks\n        &gt;&gt;&gt; drift = nn.Sequential(nn.Linear(2, 16), nn.Tanh(), nn.Linear(16, 2))\n        &gt;&gt;&gt; diffusion = nn.Sequential(nn.Linear(2, 16), nn.Tanh(), nn.Linear(16, 2))\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create fSDE with learnable Hurst parameter\n        &gt;&gt;&gt; fsde = NeuralFSDE(\n        ...     state_size=2,\n        ...     drift_net=drift,\n        ...     diffusion_net=diffusion,\n        ...     H_init=0.7,\n        ...     learnable_H=True\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Integrate from initial conditions\n        &gt;&gt;&gt; x0 = torch.randn(32, 2)  # Batch of initial states\n        &gt;&gt;&gt; trajectory = fsde(x0, n_steps=100)  # (32, 101, 2)\n    \"\"\"\n\n    def __init__(\n        self,\n        state_size,\n        drift_net,\n        diffusion_net,\n        H_init=0.5,\n        learnable_H=False,\n        t_max=1.0,\n    ):\n        super().__init__()\n        self.state_size = state_size\n        self.drift_net = drift_net\n        self.diffusion_net = diffusion_net\n        self.t_max = t_max\n\n        # Constraint: H must be in (0, 1)\n\n        raw_h = torch.tensor(H_init, dtype=torch.float32).logit()\n\n        if learnable_H:\n            self.raw_h = nn.Parameter(raw_h)\n        else:\n            self.register_buffer(\"raw_h\", raw_h)\n\n    @property\n    def H(self):\n        \"\"\"Current Hurst parameter value, constrained to (0.01, 0.99).\"\"\"\n        return torch.sigmoid(self.raw_h) * 0.98 + 0.01\n\n    def forward(self, x0, n_steps, method=\"davies_harte\"):\n        \"\"\"Integrate the fSDE from initial conditions.\n\n        Args:\n            x0: Initial state tensor of shape ``(batch, state_size)``.\n            n_steps: Number of integration steps.\n            method: fGn generation method ('davies_harte' or 'cholesky').\n\n        Returns:\n            Trajectory tensor of shape ``(batch, n_steps+1, state_size)``.\n\n        Raises:\n            ValueError: If $H &lt; 0.5$ (rough regime not yet supported).\n        \"\"\"\n        # Validate H\n        h_curr = self.H\n        if h_curr &lt; 0.5:\n             # Euler-Maruyama unstable for H &lt; 0.5\n            raise ValueError(\n                \"Standard Euler-Maruyama solvers are mathematically unstable for H &lt; 0.5 \"\n                \"(Rough paths). Please use H &gt;= 0.5 or wait for stable release (Rough Signatures).\"\n            )\n            pass \n\n        batch_size = x0.shape[0]\n        dt = self.t_max / n_steps\n        device = x0.device\n\n        # 2. Generate Noise\n        # use Torch ops for H gradient to flow\n        fgn = generate_davies_harte(\n            n_steps, h_curr, size=(batch_size, self.state_size), device=device\n        )\n\n        # 3. Scale Noise: fGn ~ N(0, 1) -&gt; N(0, dt^(2H))\n        noise_increments = fgn * (dt ** h_curr)\n\n        # 4. Integrate\n        xt = x0\n        trajectory = [x0]\n\n        #eventual optimization: use jit or vectorized ops\n        for i in range(n_steps):\n            drift = self.drift_net(xt) * dt\n            diffusion = self.diffusion_net(xt)\n\n            # SDE: dX = mu*dt + sigma*dB\n            # Using ito interpretation\n            noise = diffusion * noise_increments[..., i]\n\n            xt = xt + drift + noise\n            trajectory.append(xt)\n\n        return torch.stack(trajectory, dim=1)\n</code></pre>"},{"location":"api/#torchfbm.sde.NeuralFSDE--define-drift-and-diffusion-networks","title":"Define drift and diffusion networks","text":"<p>drift = nn.Sequential(nn.Linear(2, 16), nn.Tanh(), nn.Linear(16, 2)) diffusion = nn.Sequential(nn.Linear(2, 16), nn.Tanh(), nn.Linear(16, 2))</p>"},{"location":"api/#torchfbm.sde.NeuralFSDE--create-fsde-with-learnable-hurst-parameter","title":"Create fSDE with learnable Hurst parameter","text":"<p>fsde = NeuralFSDE( ...     state_size=2, ...     drift_net=drift, ...     diffusion_net=diffusion, ...     H_init=0.7, ...     learnable_H=True ... )</p>"},{"location":"api/#torchfbm.sde.NeuralFSDE--integrate-from-initial-conditions","title":"Integrate from initial conditions","text":"<p>x0 = torch.randn(32, 2)  # Batch of initial states trajectory = fsde(x0, n_steps=100)  # (32, 101, 2)</p>"},{"location":"api/#torchfbm.sde.NeuralFSDE.H","title":"<code>H</code>  <code>property</code>","text":"<p>Current Hurst parameter value, constrained to (0.01, 0.99).</p>"},{"location":"api/#torchfbm.sde.NeuralFSDE.forward","title":"<code>forward(x0, n_steps, method='davies_harte')</code>","text":"<p>Integrate the fSDE from initial conditions.</p> <p>Parameters:</p> Name Type Description Default <code>x0</code> <p>Initial state tensor of shape <code>(batch, state_size)</code>.</p> required <code>n_steps</code> <p>Number of integration steps.</p> required <code>method</code> <p>fGn generation method ('davies_harte' or 'cholesky').</p> <code>'davies_harte'</code> <p>Returns:</p> Type Description <p>Trajectory tensor of shape <code>(batch, n_steps+1, state_size)</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If \\(H &lt; 0.5\\) (rough regime not yet supported).</p> Source code in <code>torchfbm/sde.py</code> <pre><code>def forward(self, x0, n_steps, method=\"davies_harte\"):\n    \"\"\"Integrate the fSDE from initial conditions.\n\n    Args:\n        x0: Initial state tensor of shape ``(batch, state_size)``.\n        n_steps: Number of integration steps.\n        method: fGn generation method ('davies_harte' or 'cholesky').\n\n    Returns:\n        Trajectory tensor of shape ``(batch, n_steps+1, state_size)``.\n\n    Raises:\n        ValueError: If $H &lt; 0.5$ (rough regime not yet supported).\n    \"\"\"\n    # Validate H\n    h_curr = self.H\n    if h_curr &lt; 0.5:\n         # Euler-Maruyama unstable for H &lt; 0.5\n        raise ValueError(\n            \"Standard Euler-Maruyama solvers are mathematically unstable for H &lt; 0.5 \"\n            \"(Rough paths). Please use H &gt;= 0.5 or wait for stable release (Rough Signatures).\"\n        )\n        pass \n\n    batch_size = x0.shape[0]\n    dt = self.t_max / n_steps\n    device = x0.device\n\n    # 2. Generate Noise\n    # use Torch ops for H gradient to flow\n    fgn = generate_davies_harte(\n        n_steps, h_curr, size=(batch_size, self.state_size), device=device\n    )\n\n    # 3. Scale Noise: fGn ~ N(0, 1) -&gt; N(0, dt^(2H))\n    noise_increments = fgn * (dt ** h_curr)\n\n    # 4. Integrate\n    xt = x0\n    trajectory = [x0]\n\n    #eventual optimization: use jit or vectorized ops\n    for i in range(n_steps):\n        drift = self.drift_net(xt) * dt\n        diffusion = self.diffusion_net(xt)\n\n        # SDE: dX = mu*dt + sigma*dB\n        # Using ito interpretation\n        noise = diffusion * noise_increments[..., i]\n\n        xt = xt + drift + noise\n        trajectory.append(xt)\n\n    return torch.stack(trajectory, dim=1)\n</code></pre>"},{"location":"api/#loss-functions","title":"Loss Functions","text":"<p>Loss functions for enforcing fractal properties.</p>"},{"location":"api/#torchfbm.loss","title":"<code>torchfbm.loss</code>","text":"<p>Loss functions for enforcing fractional properties.</p> <p>This module provides regularization losses that encourage neural network outputs to exhibit specific fractional Brownian motion properties.</p> These losses are useful for <ul> <li>Generative models that should produce fractal-like outputs</li> <li>Time series forecasting with memory preservation</li> <li>Physics-informed neural networks with scaling constraints</li> </ul> Example <p>from torchfbm.loss import HurstRegularizationLoss, SpectralConsistencyLoss hurst_loss = HurstRegularizationLoss(target_H=0.7) spectral_loss = SpectralConsistencyLoss(target_beta=2.4)  # beta = 2H + 1</p>"},{"location":"api/#torchfbm.loss.HurstRegularizationLoss","title":"<code>HurstRegularizationLoss</code>","text":"<p>               Bases: <code>Module</code></p> <p>Regularization loss penalizing deviation from target Hurst exponent.</p> <p>Encourages generated time series to have a specific Hurst parameter by adding a penalty term to the training objective.</p> <p>The loss is computed as:</p> \\[\\mathcal{L}_{Hurst} = \\lambda (\\hat{H}(x) - H_{target})^2\\] <p>where \\(\\hat{H}(x)\\) is the estimated Hurst exponent of the input.</p> Usage in Training <p>Combine with task loss for multi-objective optimization:</p> <p>total_loss = mse_loss(pred, target) + 0.1 * hurst_reg(pred)</p> Note <p>The Hurst estimation uses R/S analysis, which may not be fully differentiable. Consider using :class:<code>SpectralConsistencyLoss</code> for smoother gradients.</p> <p>Parameters:</p> Name Type Description Default <code>target_H</code> <code>float</code> <p>Target Hurst exponent to encourage. Typically in (0, 1).</p> <code>0.5</code> Example <p>reg = HurstRegularizationLoss(target_H=0.7) x = torch.randn(32, 100)  # Batch of sequences loss = reg(x)  # Penalty for deviation from H=0.7</p> Source code in <code>torchfbm/loss.py</code> <pre><code>class HurstRegularizationLoss(torch.nn.Module):\n    \"\"\"Regularization loss penalizing deviation from target Hurst exponent.\n\n    Encourages generated time series to have a specific Hurst parameter\n    by adding a penalty term to the training objective.\n\n    The loss is computed as:\n\n    $$\\\\mathcal{L}_{Hurst} = \\\\lambda (\\\\hat{H}(x) - H_{target})^2$$\n\n    where $\\\\hat{H}(x)$ is the estimated Hurst exponent of the input.\n\n    Usage in Training:\n        Combine with task loss for multi-objective optimization:\n\n        &gt;&gt;&gt; total_loss = mse_loss(pred, target) + 0.1 * hurst_reg(pred)\n\n    Note:\n        The Hurst estimation uses R/S analysis, which may not be fully\n        differentiable. Consider using :class:`SpectralConsistencyLoss`\n        for smoother gradients.\n\n    Args:\n        target_H: Target Hurst exponent to encourage. Typically in (0, 1).\n\n    Example:\n        &gt;&gt;&gt; reg = HurstRegularizationLoss(target_H=0.7)\n        &gt;&gt;&gt; x = torch.randn(32, 100)  # Batch of sequences\n        &gt;&gt;&gt; loss = reg(x)  # Penalty for deviation from H=0.7\n    \"\"\"\n\n    def __init__(self, target_H: float = 0.5):\n        super().__init__()\n        self.target_H = target_H\n\n    def forward(self, x):\n        \"\"\"Compute Hurst regularization loss.\n\n        Args:\n            x: Input tensor of shape ``(batch, time)`` or ``(time,)``.\n\n        Returns:\n            Scalar loss tensor.\n        \"\"\"\n        h_est = estimate_hurst(x)\n        #Use spectral consistency for more stable differentiation\n        return torch.mean((h_est - self.target_H) ** 2)\n</code></pre>"},{"location":"api/#torchfbm.loss.HurstRegularizationLoss.forward","title":"<code>forward(x)</code>","text":"<p>Compute Hurst regularization loss.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <p>Input tensor of shape <code>(batch, time)</code> or <code>(time,)</code>.</p> required <p>Returns:</p> Type Description <p>Scalar loss tensor.</p> Source code in <code>torchfbm/loss.py</code> <pre><code>def forward(self, x):\n    \"\"\"Compute Hurst regularization loss.\n\n    Args:\n        x: Input tensor of shape ``(batch, time)`` or ``(time,)``.\n\n    Returns:\n        Scalar loss tensor.\n    \"\"\"\n    h_est = estimate_hurst(x)\n    #Use spectral consistency for more stable differentiation\n    return torch.mean((h_est - self.target_H) ** 2)\n</code></pre>"},{"location":"api/#torchfbm.loss.SpectralConsistencyLoss","title":"<code>SpectralConsistencyLoss</code>","text":"<p>               Bases: <code>Module</code></p> <p>Spectral loss for enforcing power-law scaling.</p> <p>Penalizes deviations from the target spectral exponent \\(\\beta\\), where the power spectral density follows:</p> \\[S(f) \\propto \\frac{1}{f^\\beta}\\] <p>For fractional Brownian motion, \\(\\beta = 2H + 1\\), so: - \\(H = 0.5\\) (Brownian): \\(\\beta = 2.0\\) - \\(H = 0.7\\) (Persistent): \\(\\beta = 2.4\\) - \\(H = 0.3\\) (Anti-persistent): \\(\\beta = 1.6\\)</p> Features <ul> <li>PSD smoothing: Reduces variance in spectral estimate</li> <li>Frequency masking: Ignores DC and Nyquist components</li> <li>Windowing: Hann window reduces spectral leakage</li> <li>Differentiable: Gradients flow through regression</li> </ul> Algorithm <ol> <li>Apply Hann window and compute FFT</li> <li>Estimate power spectral density</li> <li>Smooth PSD with average pooling</li> <li>Mask to relevant frequency range</li> <li>Linear regression in log-log space to estimate \\(\\beta\\)</li> <li>MSE loss against target \\(\\beta\\)</li> </ol> <p>Parameters:</p> Name Type Description Default <code>target_beta</code> <code>float</code> <p>Target spectral exponent. For fBm: \\(\\beta = 2H + 1\\).</p> required <code>low_freq_cutoff</code> <code>float</code> <p>Normalized frequency below which to ignore (avoids DC).</p> <code>0.02</code> <code>high_freq_cutoff</code> <code>float</code> <p>Normalized frequency above which to ignore (avoids noise).</p> <code>0.9</code> <code>smooth_kernel</code> <code>int</code> <p>Kernel size for PSD smoothing.</p> <code>5</code> Example <p>loss_fn = SpectralConsistencyLoss(target_beta=2.4)  # H=0.7 x = fbm(1000, H=0.7, size=(32,)) loss = loss_fn(x)  # Should be small for correct H</p> Source code in <code>torchfbm/loss.py</code> <pre><code>class SpectralConsistencyLoss(nn.Module):\n    \"\"\"Spectral loss for enforcing power-law scaling.\n\n    Penalizes deviations from the target spectral exponent $\\\\beta$,\n    where the power spectral density follows:\n\n    $$S(f) \\\\propto \\\\frac{1}{f^\\\\beta}$$\n\n    For fractional Brownian motion, $\\\\beta = 2H + 1$, so:\n    - $H = 0.5$ (Brownian): $\\\\beta = 2.0$\n    - $H = 0.7$ (Persistent): $\\\\beta = 2.4$\n    - $H = 0.3$ (Anti-persistent): $\\\\beta = 1.6$\n\n    Features:\n        - **PSD smoothing**: Reduces variance in spectral estimate\n        - **Frequency masking**: Ignores DC and Nyquist components\n        - **Windowing**: Hann window reduces spectral leakage\n        - **Differentiable**: Gradients flow through regression\n\n    Algorithm:\n        1. Apply Hann window and compute FFT\n        2. Estimate power spectral density\n        3. Smooth PSD with average pooling\n        4. Mask to relevant frequency range\n        5. Linear regression in log-log space to estimate $\\\\beta$\n        6. MSE loss against target $\\\\beta$\n\n    Args:\n        target_beta: Target spectral exponent. For fBm: $\\\\beta = 2H + 1$.\n        low_freq_cutoff: Normalized frequency below which to ignore (avoids DC).\n        high_freq_cutoff: Normalized frequency above which to ignore (avoids noise).\n        smooth_kernel: Kernel size for PSD smoothing.\n\n    Example:\n        &gt;&gt;&gt; loss_fn = SpectralConsistencyLoss(target_beta=2.4)  # H=0.7\n        &gt;&gt;&gt; x = fbm(1000, H=0.7, size=(32,))\n        &gt;&gt;&gt; loss = loss_fn(x)  # Should be small for correct H\n    \"\"\"\n\n    def __init__(\n        self, \n        target_beta: float, \n        low_freq_cutoff: float = 0.02, \n        high_freq_cutoff: float = 0.9, \n        smooth_kernel: int = 5\n    ):\n        super().__init__()\n        self.target_beta = target_beta\n        self.low_freq_cutoff = low_freq_cutoff\n        self.high_freq_cutoff = high_freq_cutoff\n        self.smooth_kernel = smooth_kernel\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"Compute spectral consistency loss.\n\n        Args:\n            x: Input tensor of shape ``(batch, channels, time)`` or ``(batch, time)``.\n\n        Returns:\n            Scalar MSE loss between estimated and target beta.\n        \"\"\"\n        # x shape: (Batch, Channels, Time) or (Batch, Time)\n        n = x.shape[-1]\n        device = x.device\n\n        # 1. Compute PSD with Windowing (to reduce leakage)\n        # Hann window prevents the \"cliff-edge\" effect at the ends of the sequence\n        window = torch.hann_window(n, periodic=True, device=device)\n        fft = torch.fft.rfft(x * window, dim=-1)\n        psd = torch.abs(fft) ** 2\n\n        # 2. Smooth the PSD\n        # Raw periodograms are too noisy for stable gradients. \n        # We apply a 1D average pool to smooth the spectral estimate.\n        if self.smooth_kernel &gt; 1:\n            psd = F.avg_pool1d(\n                psd.view(-1, 1, psd.shape[-1]), \n                kernel_size=self.smooth_kernel, \n                stride=1, \n                padding=self.smooth_kernel // 2\n            ).view(psd.shape)\n\n        # 3. Frequency setup\n        freqs = torch.fft.rfftfreq(n, d=1.0, device=device)\n\n        # 4. Create Mask\n        # Skip DC component and extreme high-frequency noise\n        mask = (freqs &gt; self.low_freq_cutoff) &amp; (freqs &lt; self.high_freq_cutoff)\n        # Ensure at least some bins remain\n        if not mask.any():\n            return torch.tensor(0.0, device=device, requires_grad=True)\n\n        log_f = torch.log(freqs[mask])\n        log_psd = torch.log(psd[..., mask] + 1e-10)\n\n        # 5. Batch Regression\n        # We solve: log_psd = -beta * log_f + intercept\n        # Formula for slope: beta = - Cov(log_f, log_psd) / Var(log_f)\n        mean_f = log_f.mean()\n        mean_psd = log_psd.mean(dim=-1, keepdim=True)\n\n        diff_f = log_f - mean_f\n        diff_psd = log_psd - mean_psd\n\n        num = (diff_f * diff_psd).sum(dim=-1)\n        den = (diff_f ** 2).sum()\n\n        estimated_beta = -num / (den + 1e-8)\n\n        # 6. Penalize deviations from target\n        return F.mse_loss(estimated_beta, torch.full_like(estimated_beta, self.target_beta))\n</code></pre>"},{"location":"api/#torchfbm.loss.SpectralConsistencyLoss.forward","title":"<code>forward(x)</code>","text":"<p>Compute spectral consistency loss.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape <code>(batch, channels, time)</code> or <code>(batch, time)</code>.</p> required <p>Returns:</p> Type Description <p>Scalar MSE loss between estimated and target beta.</p> Source code in <code>torchfbm/loss.py</code> <pre><code>def forward(self, x: torch.Tensor):\n    \"\"\"Compute spectral consistency loss.\n\n    Args:\n        x: Input tensor of shape ``(batch, channels, time)`` or ``(batch, time)``.\n\n    Returns:\n        Scalar MSE loss between estimated and target beta.\n    \"\"\"\n    # x shape: (Batch, Channels, Time) or (Batch, Time)\n    n = x.shape[-1]\n    device = x.device\n\n    # 1. Compute PSD with Windowing (to reduce leakage)\n    # Hann window prevents the \"cliff-edge\" effect at the ends of the sequence\n    window = torch.hann_window(n, periodic=True, device=device)\n    fft = torch.fft.rfft(x * window, dim=-1)\n    psd = torch.abs(fft) ** 2\n\n    # 2. Smooth the PSD\n    # Raw periodograms are too noisy for stable gradients. \n    # We apply a 1D average pool to smooth the spectral estimate.\n    if self.smooth_kernel &gt; 1:\n        psd = F.avg_pool1d(\n            psd.view(-1, 1, psd.shape[-1]), \n            kernel_size=self.smooth_kernel, \n            stride=1, \n            padding=self.smooth_kernel // 2\n        ).view(psd.shape)\n\n    # 3. Frequency setup\n    freqs = torch.fft.rfftfreq(n, d=1.0, device=device)\n\n    # 4. Create Mask\n    # Skip DC component and extreme high-frequency noise\n    mask = (freqs &gt; self.low_freq_cutoff) &amp; (freqs &lt; self.high_freq_cutoff)\n    # Ensure at least some bins remain\n    if not mask.any():\n        return torch.tensor(0.0, device=device, requires_grad=True)\n\n    log_f = torch.log(freqs[mask])\n    log_psd = torch.log(psd[..., mask] + 1e-10)\n\n    # 5. Batch Regression\n    # We solve: log_psd = -beta * log_f + intercept\n    # Formula for slope: beta = - Cov(log_f, log_psd) / Var(log_f)\n    mean_f = log_f.mean()\n    mean_psd = log_psd.mean(dim=-1, keepdim=True)\n\n    diff_f = log_f - mean_f\n    diff_psd = log_psd - mean_psd\n\n    num = (diff_f * diff_psd).sum(dim=-1)\n    den = (diff_f ** 2).sum()\n\n    estimated_beta = -num / (den + 1e-8)\n\n    # 6. Penalize deviations from target\n    return F.mse_loss(estimated_beta, torch.full_like(estimated_beta, self.target_beta))\n</code></pre>"},{"location":"api/#schedulers","title":"Schedulers","text":"<p>Hurst parameter scheduling for diffusion models.</p>"},{"location":"api/#torchfbm.schedulers","title":"<code>torchfbm.schedulers</code>","text":"<p>Hurst parameter schedules for diffusion models.</p> <p>This module provides annealing schedules for the Hurst parameter, useful in fractional diffusion models where the roughness of the noise varies across sampling steps.</p> <p>Inspired by noise scheduling in denoising diffusion models (Ho, Jain &amp; Abbeel, 2020).</p> Example <p>from torchfbm.schedulers import get_hurst_schedule</p>"},{"location":"api/#torchfbm.schedulers--smooth-start-rough-end","title":"Smooth start, rough end","text":"<p>schedule = get_hurst_schedule(100, start_H=0.7, end_H=0.3, type='cosine')</p>"},{"location":"api/#torchfbm.schedulers.get_hurst_schedule","title":"<code>get_hurst_schedule(n_steps, start_H=0.3, end_H=0.7, type='linear')</code>","text":"<p>Generate a schedule of Hurst parameters for diffusion sampling.</p> <p>Creates an annealing schedule that varies the Hurst parameter across diffusion time steps. This allows for adaptive noise roughness during the generation process.</p> Schedule Types <ul> <li> <p>linear: Uniform interpolation between start and end values</p> \\[H_t = start_H + \\frac{t}{T}(end_H - start_H)\\] </li> <li> <p>cosine: Smooth cosine annealing (slower changes at endpoints)</p> \\[H_t = end_H + \\frac{1}{2}(start_H - end_H)(1 + \\cos(\\frac{\\pi t}{T}))\\] </li> </ul> Applications <ul> <li>Diffusion models: Varying noise roughness during denoising</li> <li>Curriculum learning: Gradually changing correlation structure</li> <li>Annealing: Smooth transition between memory regimes</li> </ul> <p>Parameters:</p> Name Type Description Default <code>n_steps</code> <code>int</code> <p>Total number of steps in the schedule.</p> required <code>start_H</code> <code>float</code> <p>Initial Hurst parameter value.</p> <code>0.3</code> <code>end_H</code> <code>float</code> <p>Final Hurst parameter value.</p> <code>0.7</code> <code>type</code> <p>Schedule type, either 'linear' or 'cosine'.</p> <code>'linear'</code> <p>Returns:</p> Type Description <p>Tensor of shape <code>(n_steps,)</code> with Hurst values.</p> Example Source code in <code>torchfbm/schedulers.py</code> <pre><code>def get_hurst_schedule(\n    n_steps: int, start_H: float = 0.3, end_H: float = 0.7, type=\"linear\"\n):\n    \"\"\"Generate a schedule of Hurst parameters for diffusion sampling.\n\n    Creates an annealing schedule that varies the Hurst parameter across\n    diffusion time steps. This allows for adaptive noise roughness during\n    the generation process.\n\n    Schedule Types:\n        - **linear**: Uniform interpolation between start and end values\n\n            $$H_t = start_H + \\\\frac{t}{T}(end_H - start_H)$$\n\n        - **cosine**: Smooth cosine annealing (slower changes at endpoints)\n\n            $$H_t = end_H + \\\\frac{1}{2}(start_H - end_H)(1 + \\\\cos(\\\\frac{\\\\pi t}{T}))$$\n\n    Applications:\n        - **Diffusion models**: Varying noise roughness during denoising\n        - **Curriculum learning**: Gradually changing correlation structure\n        - **Annealing**: Smooth transition between memory regimes\n\n    Args:\n        n_steps: Total number of steps in the schedule.\n        start_H: Initial Hurst parameter value.\n        end_H: Final Hurst parameter value.\n        type: Schedule type, either 'linear' or 'cosine'.\n\n    Returns:\n        Tensor of shape ``(n_steps,)`` with Hurst values.\n\n    Example:\n        &gt;&gt;&gt; # Linear schedule from rough to smooth\n        &gt;&gt;&gt; h_linear = get_hurst_schedule(100, start_H=0.3, end_H=0.7, type='linear')\n        &gt;&gt;&gt; h_linear[0], h_linear[-1]  # 0.3, 0.7\n\n        &gt;&gt;&gt; # Cosine schedule (slower at endpoints)\n        &gt;&gt;&gt; h_cosine = get_hurst_schedule(100, start_H=0.3, end_H=0.7, type='cosine')\n    \"\"\"\n    if type == \"linear\":\n        return torch.linspace(start_H, end_H, n_steps)\n    elif type == \"cosine\":\n        steps = torch.arange(n_steps)\n        return end_H + 0.5 * (start_H - end_H) * (\n            1 + torch.cos(steps / n_steps * torch.pi)\n        )\n</code></pre>"},{"location":"api/#torchfbm.schedulers.get_hurst_schedule--linear-schedule-from-rough-to-smooth","title":"Linear schedule from rough to smooth","text":"<p>h_linear = get_hurst_schedule(100, start_H=0.3, end_H=0.7, type='linear') h_linear[0], h_linear[-1]  # 0.3, 0.7</p>"},{"location":"api/#torchfbm.schedulers.get_hurst_schedule--cosine-schedule-slower-at-endpoints","title":"Cosine schedule (slower at endpoints)","text":"<p>h_cosine = get_hurst_schedule(100, start_H=0.3, end_H=0.7, type='cosine')</p>"},{"location":"api/#augmentations","title":"Augmentations","text":"<p>Data augmentation with fractional noise.</p>"},{"location":"api/#torchfbm.augmentations","title":"<code>torchfbm.augmentations</code>","text":"<p>Data augmentation with fractional Gaussian noise.</p> <p>This module provides augmentation techniques that add correlated noise to training data, helping models become robust to specific correlation structures in input data.</p> Example <p>from torchfbm.augmentations import FractionalNoiseAugmentation augment = FractionalNoiseAugmentation(H=0.7, sigma=0.01) x_augmented = augment(x)</p>"},{"location":"api/#torchfbm.augmentations.FractionalNoiseAugmentation","title":"<code>FractionalNoiseAugmentation</code>","text":"<p>               Bases: <code>Module</code></p> <p>Data augmentation by adding fractional Gaussian noise.</p> <p>Adds fGn with specified Hurst parameter to training samples, helping models become robust to different correlation structures.</p> <p>The augmentation adds noise scaled by sigma:</p> \\[x_{aug} = x + \\sigma \\cdot fGn(H)\\] Use Cases <ul> <li>H &gt; 0.5 (persistent): Robustness to trending patterns</li> <li>H &lt; 0.5 (anti-persistent): Robustness to mean-reverting noise</li> <li>H = 0.5: Equivalent to standard Gaussian noise augmentation</li> </ul> <p>The augmentation is only applied during training with probability <code>p</code>.</p> <p>Parameters:</p> Name Type Description Default <code>H</code> <code>float</code> <p>Hurst parameter for the fGn. Controls correlation structure.</p> <code>0.5</code> <code>sigma</code> <code>float</code> <p>Noise amplitude scaling factor.</p> <code>0.01</code> <code>p</code> <code>float</code> <p>Probability of applying the augmentation (per sample).</p> <code>0.5</code> Example Source code in <code>torchfbm/augmentations.py</code> <pre><code>class FractionalNoiseAugmentation(torch.nn.Module):\n    \"\"\"Data augmentation by adding fractional Gaussian noise.\n\n    Adds fGn with specified Hurst parameter to training samples,\n    helping models become robust to different correlation structures.\n\n    The augmentation adds noise scaled by sigma:\n\n    $$x_{aug} = x + \\\\sigma \\\\cdot fGn(H)$$\n\n    Use Cases:\n        - **H &gt; 0.5 (persistent)**: Robustness to trending patterns\n        - **H &lt; 0.5 (anti-persistent)**: Robustness to mean-reverting noise\n        - **H = 0.5**: Equivalent to standard Gaussian noise augmentation\n\n    The augmentation is only applied during training with probability ``p``.\n\n    Args:\n        H: Hurst parameter for the fGn. Controls correlation structure.\n        sigma: Noise amplitude scaling factor.\n        p: Probability of applying the augmentation (per sample).\n\n    Example:\n        &gt;&gt;&gt; # Make model robust to trending noise\n        &gt;&gt;&gt; augment = FractionalNoiseAugmentation(H=0.7, sigma=0.01, p=0.5)\n        &gt;&gt;&gt; model = nn.Sequential(augment, nn.Linear(100, 10))\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # During training, 50% of samples get fGn added\n        &gt;&gt;&gt; model.train()\n        &gt;&gt;&gt; y = model(x)  # Augmentation active\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # During eval, no augmentation\n        &gt;&gt;&gt; model.eval()\n        &gt;&gt;&gt; y = model(x)  # No noise added\n    \"\"\"\n\n    def __init__(self, H: float = 0.5, sigma: float = 0.01, p: float = 0.5):\n        super().__init__()\n        self.H = H\n        self.sigma = sigma\n        self.p = p\n\n    def forward(self, x):\n        \"\"\"Apply fractional noise augmentation.\n\n        Args:\n            x: Input tensor. Noise is added along the last dimension.\n\n        Returns:\n            Augmented tensor (during training with probability p),\n            or unchanged input (during eval or with probability 1-p).\n        \"\"\"\n        if self.training and torch.rand(1) &lt; self.p:\n            noise = generate_davies_harte(\n                x.shape[-1], self.H, size=x.shape[:-1], device=x.device\n            )\n            return x + self.sigma * noise\n        return x\n</code></pre>"},{"location":"api/#torchfbm.augmentations.FractionalNoiseAugmentation--make-model-robust-to-trending-noise","title":"Make model robust to trending noise","text":"<p>augment = FractionalNoiseAugmentation(H=0.7, sigma=0.01, p=0.5) model = nn.Sequential(augment, nn.Linear(100, 10))</p>"},{"location":"api/#torchfbm.augmentations.FractionalNoiseAugmentation--during-training-50-of-samples-get-fgn-added","title":"During training, 50% of samples get fGn added","text":"<p>model.train() y = model(x)  # Augmentation active</p>"},{"location":"api/#torchfbm.augmentations.FractionalNoiseAugmentation--during-eval-no-augmentation","title":"During eval, no augmentation","text":"<p>model.eval() y = model(x)  # No noise added</p>"},{"location":"api/#torchfbm.augmentations.FractionalNoiseAugmentation.forward","title":"<code>forward(x)</code>","text":"<p>Apply fractional noise augmentation.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <p>Input tensor. Noise is added along the last dimension.</p> required <p>Returns:</p> Type Description <p>Augmented tensor (during training with probability p),</p> <p>or unchanged input (during eval or with probability 1-p).</p> Source code in <code>torchfbm/augmentations.py</code> <pre><code>def forward(self, x):\n    \"\"\"Apply fractional noise augmentation.\n\n    Args:\n        x: Input tensor. Noise is added along the last dimension.\n\n    Returns:\n        Augmented tensor (during training with probability p),\n        or unchanged input (during eval or with probability 1-p).\n    \"\"\"\n    if self.training and torch.rand(1) &lt; self.p:\n        noise = generate_davies_harte(\n            x.shape[-1], self.H, size=x.shape[:-1], device=x.device\n        )\n        return x + self.sigma * noise\n    return x\n</code></pre>"},{"location":"api/#online","title":"Online","text":"<p>Streaming/online generation of fGn.</p>"},{"location":"api/#torchfbm.online","title":"<code>torchfbm.online</code>","text":"<p>Online (streaming) fractional Gaussian noise generation.</p> <p>This module provides incremental generation of fGn samples, useful when the sequence length is not known in advance or when memory constraints prevent batch generation.</p> <p>Based on the incremental Cholesky update approach from Dietrich &amp; Newsam (1997).</p> Example <p>from torchfbm.online import CachedFGNGenerator gen = CachedFGNGenerator(H=0.7) samples = [gen.step() for _ in range(100)]</p>"},{"location":"api/#torchfbm.online.CachedFGNGenerator","title":"<code>CachedFGNGenerator</code>","text":"<p>Online fractional Gaussian noise generator with incremental Cholesky updates.</p> <p>Based on Dietrich &amp; Newsam (1997).</p> <p>Generates exact fGn samples one at a time, maintaining the correct correlation structure by incrementally updating the Cholesky factorization.</p> Algorithm <p>At step \\(n\\), we have the covariance matrix \\(\\Sigma_n\\) and its Cholesky factor \\(L_n\\) such that \\(\\Sigma_n = L_n L_n^T\\).</p> <p>To add a new sample: 1. Compute the new covariance row \\(\\mathbf{v} = Cov(X_{n+1}, X_1, ..., X_n)\\) 2. Solve \\(L_n \\mathbf{w} = \\mathbf{v}\\) for \\(\\mathbf{w}\\) 3. Compute \\(\\delta = \\sqrt{\\gamma(0) - \\|\\mathbf{w}\\|^2}\\) 4. Generate \\(X_{n+1} = \\mathbf{w}^T \\mathbf{Z} + \\delta z_{new}\\) 5. Extend \\(L_{n+1}\\) with the new row</p> Complexity <ul> <li>Per step: \\(O(n)\\) for solve and update</li> <li>Total for \\(N\\) steps: \\(O(N^2)\\)</li> <li>Memory: \\(O(N^2)\\) for Cholesky factor</li> </ul> Note <p>For batch generation where the full length is known, use :func:<code>generate_cholesky</code> or :func:<code>generate_davies_harte</code> instead.</p> <p>Parameters:</p> Name Type Description Default <code>H</code> <code>float</code> <p>Hurst parameter in (0, 1).</p> required <code>device</code> <p>Computation device ('cpu' or 'cuda').</p> <code>'cpu'</code> <code>dtype</code> <p>Data type for tensors.</p> <code>float32</code> <p>Attributes:</p> Name Type Description <code>n</code> <p>Current number of generated samples.</p> <code>H</code> <p>Hurst parameter.</p> Example <p>gen = CachedFGNGenerator(H=0.7, device='cpu')</p> Source code in <code>torchfbm/online.py</code> <pre><code>class CachedFGNGenerator:\n    \"\"\"Online fractional Gaussian noise generator with incremental Cholesky updates.\n\n    Based on Dietrich &amp; Newsam (1997).\n\n    Generates exact fGn samples one at a time, maintaining the correct\n    correlation structure by incrementally updating the Cholesky factorization.\n\n    Algorithm:\n        At step $n$, we have the covariance matrix $\\\\Sigma_n$ and its\n        Cholesky factor $L_n$ such that $\\\\Sigma_n = L_n L_n^T$.\n\n        To add a new sample:\n        1. Compute the new covariance row $\\\\mathbf{v} = Cov(X_{n+1}, X_1, ..., X_n)$\n        2. Solve $L_n \\\\mathbf{w} = \\\\mathbf{v}$ for $\\\\mathbf{w}$\n        3. Compute $\\\\delta = \\\\sqrt{\\\\gamma(0) - \\\\|\\\\mathbf{w}\\\\|^2}$\n        4. Generate $X_{n+1} = \\\\mathbf{w}^T \\\\mathbf{Z} + \\\\delta z_{new}$\n        5. Extend $L_{n+1}$ with the new row\n\n    Complexity:\n        - Per step: $O(n)$ for solve and update\n        - Total for $N$ steps: $O(N^2)$\n        - Memory: $O(N^2)$ for Cholesky factor\n\n    Note:\n        For batch generation where the full length is known, use\n        :func:`generate_cholesky` or :func:`generate_davies_harte` instead.\n\n    Args:\n        H: Hurst parameter in (0, 1).\n        device: Computation device ('cpu' or 'cuda').\n        dtype: Data type for tensors.\n\n    Attributes:\n        n: Current number of generated samples.\n        H: Hurst parameter.\n\n    Example:\n        &gt;&gt;&gt; gen = CachedFGNGenerator(H=0.7, device='cpu')\n        &gt;&gt;&gt; # Generate samples one at a time\n        &gt;&gt;&gt; for _ in range(100):\n        ...     sample = gen.step()\n        ...     process_sample(sample)\n    \"\"\"\n\n    def __init__(self, H: float, device=\"cpu\", dtype=torch.float32):\n        self.H = H\n        self.device = torch.device(device)\n        self.dtype = dtype\n        self.n = 0\n\n        self.L = torch.zeros(0, 0, device=self.device, dtype=self.dtype)\n        self.z_history = torch.zeros(0, device=self.device, dtype=self.dtype)\n\n    def step(self) -&gt; torch.Tensor:\n        \"\"\"Generate the next sample in the fGn sequence.\n\n        Each call generates a single correlated sample that maintains\n        the correct fGn covariance structure with all previous samples.\n\n        Returns:\n            Scalar tensor containing the next fGn value.\n\n        Example:\n            &gt;&gt;&gt; gen = CachedFGNGenerator(H=0.7)\n            &gt;&gt;&gt; x1 = gen.step()  # First sample\n            &gt;&gt;&gt; x2 = gen.step()  # Second sample (correlated with x1)\n        \"\"\"\n        new_idx = self.n\n        z = torch.randn(1, device=self.device, dtype=self.dtype)\n\n        if new_idx == 0:\n            # Initialize first point\n            self.L = torch.ones(1, 1, device=self.device, dtype=self.dtype)\n            val = z\n            self.z_history = torch.cat([self.z_history, z])\n\n        else:\n            # Incremental Cholesky update\n            full_gamma = _autocovariance(self.H, new_idx + 1, self.device, self.dtype)\n            v = full_gamma[1:].flip(0)\n            c = full_gamma[0]\n\n            # Solve L * w = v via forward substitution\n            v = v.unsqueeze(1)\n            w = torch.linalg.solve_triangular(self.L, v, upper=False)\n            w_flat = w.flatten()\n            w_norm_sq = torch.dot(w_flat, w_flat)\n\n            delta_sq = c - w_norm_sq\n            delta = torch.sqrt(torch.clamp(delta_sq, min=1e-8))\n\n            # Generate correlated sample: X = w^T Z + delta * z_new\n            val = torch.dot(w_flat, self.z_history) + delta * z\n            self.z_history = torch.cat([self.z_history, z])\n\n            # Extend Cholesky factor\n            zeros_col = torch.zeros(self.n, 1, device=self.device, dtype=self.dtype)\n            L_padded = torch.cat([self.L, zeros_col], dim=1)\n            bottom_row = torch.cat([w_flat, delta.unsqueeze(0)])\n            self.L = torch.vstack([L_padded, bottom_row.unsqueeze(0)])\n\n        self.n += 1\n        return val\n</code></pre>"},{"location":"api/#torchfbm.online.CachedFGNGenerator--generate-samples-one-at-a-time","title":"Generate samples one at a time","text":"<p>for _ in range(100): ...     sample = gen.step() ...     process_sample(sample)</p>"},{"location":"api/#torchfbm.online.CachedFGNGenerator.step","title":"<code>step()</code>","text":"<p>Generate the next sample in the fGn sequence.</p> <p>Each call generates a single correlated sample that maintains the correct fGn covariance structure with all previous samples.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>Scalar tensor containing the next fGn value.</p> Example <p>gen = CachedFGNGenerator(H=0.7) x1 = gen.step()  # First sample x2 = gen.step()  # Second sample (correlated with x1)</p> Source code in <code>torchfbm/online.py</code> <pre><code>def step(self) -&gt; torch.Tensor:\n    \"\"\"Generate the next sample in the fGn sequence.\n\n    Each call generates a single correlated sample that maintains\n    the correct fGn covariance structure with all previous samples.\n\n    Returns:\n        Scalar tensor containing the next fGn value.\n\n    Example:\n        &gt;&gt;&gt; gen = CachedFGNGenerator(H=0.7)\n        &gt;&gt;&gt; x1 = gen.step()  # First sample\n        &gt;&gt;&gt; x2 = gen.step()  # Second sample (correlated with x1)\n    \"\"\"\n    new_idx = self.n\n    z = torch.randn(1, device=self.device, dtype=self.dtype)\n\n    if new_idx == 0:\n        # Initialize first point\n        self.L = torch.ones(1, 1, device=self.device, dtype=self.dtype)\n        val = z\n        self.z_history = torch.cat([self.z_history, z])\n\n    else:\n        # Incremental Cholesky update\n        full_gamma = _autocovariance(self.H, new_idx + 1, self.device, self.dtype)\n        v = full_gamma[1:].flip(0)\n        c = full_gamma[0]\n\n        # Solve L * w = v via forward substitution\n        v = v.unsqueeze(1)\n        w = torch.linalg.solve_triangular(self.L, v, upper=False)\n        w_flat = w.flatten()\n        w_norm_sq = torch.dot(w_flat, w_flat)\n\n        delta_sq = c - w_norm_sq\n        delta = torch.sqrt(torch.clamp(delta_sq, min=1e-8))\n\n        # Generate correlated sample: X = w^T Z + delta * z_new\n        val = torch.dot(w_flat, self.z_history) + delta * z\n        self.z_history = torch.cat([self.z_history, z])\n\n        # Extend Cholesky factor\n        zeros_col = torch.zeros(self.n, 1, device=self.device, dtype=self.dtype)\n        L_padded = torch.cat([self.L, zeros_col], dim=1)\n        bottom_row = torch.cat([w_flat, delta.unsqueeze(0)])\n        self.L = torch.vstack([L_padded, bottom_row.unsqueeze(0)])\n\n    self.n += 1\n    return val\n</code></pre>"},{"location":"api/#reinforcement-learning","title":"Reinforcement Learning","text":"<p>Action noise for RL exploration.</p>"},{"location":"api/#torchfbm.rl","title":"<code>torchfbm.rl</code>","text":"<p>Fractional noise for reinforcement learning exploration.</p> <p>This module provides action noise generators using fractional Gaussian noise, enabling correlated exploration strategies in RL algorithms.</p> The use of fGn instead of white noise allows for <ul> <li>Smoother exploration trajectories (H &gt; 0.5)</li> <li>More thorough local exploration (H &lt; 0.5)</li> <li>Tunable temporal correlation in action perturbations</li> </ul> <p>Compatible with Stable Baselines3 and similar RL frameworks.</p> Example <p>from torchfbm.rl import FBMActionNoise noise = FBMActionNoise(mean=0, sigma=0.1, H=0.7) action = policy(state) + noise()</p>"},{"location":"api/#torchfbm.rl.FBMActionNoise","title":"<code>FBMActionNoise</code>","text":"<p>Fractional Gaussian noise for RL action space exploration.</p> <p>Generates temporally correlated noise for action exploration, providing smoother or rougher perturbations depending on the Hurst parameter.</p> <p>Inspired by Ornstein-Uhlenbeck noise commonly used in DDPG, but with controllable long-range dependence.</p> Properties by Hurst Parameter <ul> <li>H &gt; 0.5 (persistent): Smooth, trending exploration. Actions   tend to continue in the same direction, good for momentum-based tasks.</li> <li>H = 0.5: Standard Gaussian noise (memoryless).</li> <li>H &lt; 0.5 (anti-persistent): Rough, oscillating exploration.   Actions frequently reverse, good for thorough local search.</li> </ul> Implementation <p>Pre-generates a buffer of fGn samples for efficiency. When the buffer is exhausted, a new batch is generated automatically.</p> Compatibility <ul> <li>return_numpy=True: Compatible with Stable Baselines3</li> <li>return_numpy=False: Returns PyTorch tensors (for custom implementations)</li> </ul> <p>Parameters:</p> Name Type Description Default <code>mean</code> <p>Mean of the noise distribution.</p> required <code>sigma</code> <p>Standard deviation scaling factor.</p> required <code>H</code> <p>Hurst parameter in (0, 1). Controls temporal correlation.</p> <code>0.5</code> <code>size</code> <p>Shape of noise samples (action dimensions).</p> <code>(1,)</code> <code>buffer_size</code> <p>Number of pre-generated samples.</p> <code>10000</code> <code>method</code> <p>Generation method ('davies_harte' or 'cholesky').</p> <code>'davies_harte'</code> <code>device</code> <p>Computation device for tensor generation.</p> <code>'cpu'</code> <code>return_numpy</code> <p>If True, returns NumPy arrays (SB3 compatible).</p> <code>False</code> Example Source code in <code>torchfbm/rl.py</code> <pre><code>class FBMActionNoise:\n    \"\"\"Fractional Gaussian noise for RL action space exploration.\n\n    Generates temporally correlated noise for action exploration, providing\n    smoother or rougher perturbations depending on the Hurst parameter.\n\n    Inspired by Ornstein-Uhlenbeck noise commonly used in DDPG, but with\n    controllable long-range dependence.\n\n    Properties by Hurst Parameter:\n        - **H &gt; 0.5 (persistent)**: Smooth, trending exploration. Actions\n          tend to continue in the same direction, good for momentum-based tasks.\n        - **H = 0.5**: Standard Gaussian noise (memoryless).\n        - **H &lt; 0.5 (anti-persistent)**: Rough, oscillating exploration.\n          Actions frequently reverse, good for thorough local search.\n\n    Implementation:\n        Pre-generates a buffer of fGn samples for efficiency. When the buffer\n        is exhausted, a new batch is generated automatically.\n\n    Compatibility:\n        - **return_numpy=True**: Compatible with Stable Baselines3\n        - **return_numpy=False**: Returns PyTorch tensors (for custom implementations)\n\n    Args:\n        mean: Mean of the noise distribution.\n        sigma: Standard deviation scaling factor.\n        H: Hurst parameter in (0, 1). Controls temporal correlation.\n        size: Shape of noise samples (action dimensions).\n        buffer_size: Number of pre-generated samples.\n        method: Generation method ('davies_harte' or 'cholesky').\n        device: Computation device for tensor generation.\n        return_numpy: If True, returns NumPy arrays (SB3 compatible).\n\n    Example:\n        &gt;&gt;&gt; # For Stable Baselines3\n        &gt;&gt;&gt; noise = FBMActionNoise(\n        ...     mean=np.zeros(action_dim),\n        ...     sigma=0.1,\n        ...     H=0.7,\n        ...     return_numpy=True\n        ... )\n        &gt;&gt;&gt; model = DDPG(\"MlpPolicy\", env, action_noise=noise)\n\n        &gt;&gt;&gt; # For custom PyTorch RL\n        &gt;&gt;&gt; noise = FBMActionNoise(\n        ...     mean=0, sigma=0.1, H=0.6, device='cuda'\n        ... )\n        &gt;&gt;&gt; action = policy(state) + noise()\n    \"\"\"\n\n    def __init__(\n        self,\n        mean,\n        sigma,\n        H=0.5,\n        size=(1,),\n        buffer_size=10000,\n        method=\"davies_harte\",\n        device=\"cpu\",\n        return_numpy=False,\n    ):\n        self._mu = mean\n        self._sigma = sigma\n        self._H = H\n        self._size = size\n        self._buffer_size = buffer_size\n        self._method = method\n        self._device = device\n        self._return_numpy = return_numpy\n\n        self.reset()\n\n    def reset(self):\n        \"\"\"Pre-generate a buffer of fGn samples.\n\n        Called automatically during initialization and when the\n        buffer is exhausted during sampling.\n        \"\"\"\n        if self._method == \"cholesky\":\n            gen_func = generate_cholesky\n        else:\n            gen_func = generate_davies_harte\n\n        fgn = gen_func(self._buffer_size, self._H, size=self._size, device=self._device)\n\n        if self._return_numpy:\n            try:\n                self._noise_buffer = (\n                    fgn.detach().cpu().numpy()\n                )  # Convert to NumPy on CPU\n            except RuntimeError as e:\n                if \"Numpy is not available\" in str(e):\n                    raise ImportError(\n                        \"NumPy conversion requested but NumPy is not properly installed or has compatibility issues. \"\n                        \"Please install/upgrade numpy: pip install -U numpy\"\n                    ) from e\n                raise\n        else:\n            self._noise_buffer = fgn  # Keep as Tensor on Device\n\n        self._step = 0\n\n    def __call__(self):\n        \"\"\"Sample noise for the current step.\n\n        Returns:\n            Noise sample, either as NumPy array (if return_numpy=True)\n            or PyTorch tensor.\n        \"\"\"\n        if self._step &gt;= self._buffer_size:\n            self.reset()\n\n        noise = self._noise_buffer[..., self._step]\n        self._step += 1\n        val = self._mu + self._sigma * noise\n\n        if self._return_numpy:\n            if isinstance(val, torch.Tensor):\n                try:\n                    return val.detach().cpu().numpy()\n                except RuntimeError as e:\n                    if \"Numpy is not available\" in str(e):\n                        raise ImportError(\n                            \"NumPy conversion requested but NumPy is not properly installed or has compatibility issues. \"\n                            \"Please install/upgrade numpy: pip install -U numpy\"\n                        ) from e\n                    raise\n            return np.asarray(val)\n        else:\n            # If val is somehow numpy/float, cast to tensor\n            if not isinstance(val, torch.Tensor):\n                val = torch.tensor(val, device=self._device)\n            return val\n\n    def __repr__(self) -&gt; str:\n        return f\"FBMActionNoise(mu={self._mu}, sigma={self._sigma}, H={self._H}, numpy={self._return_numpy})\"\n</code></pre>"},{"location":"api/#torchfbm.rl.FBMActionNoise--for-stable-baselines3","title":"For Stable Baselines3","text":"<p>noise = FBMActionNoise( ...     mean=np.zeros(action_dim), ...     sigma=0.1, ...     H=0.7, ...     return_numpy=True ... ) model = DDPG(\"MlpPolicy\", env, action_noise=noise)</p>"},{"location":"api/#torchfbm.rl.FBMActionNoise--for-custom-pytorch-rl","title":"For custom PyTorch RL","text":"<p>noise = FBMActionNoise( ...     mean=0, sigma=0.1, H=0.6, device='cuda' ... ) action = policy(state) + noise()</p>"},{"location":"api/#torchfbm.rl.FBMActionNoise.__call__","title":"<code>__call__()</code>","text":"<p>Sample noise for the current step.</p> <p>Returns:</p> Type Description <p>Noise sample, either as NumPy array (if return_numpy=True)</p> <p>or PyTorch tensor.</p> Source code in <code>torchfbm/rl.py</code> <pre><code>def __call__(self):\n    \"\"\"Sample noise for the current step.\n\n    Returns:\n        Noise sample, either as NumPy array (if return_numpy=True)\n        or PyTorch tensor.\n    \"\"\"\n    if self._step &gt;= self._buffer_size:\n        self.reset()\n\n    noise = self._noise_buffer[..., self._step]\n    self._step += 1\n    val = self._mu + self._sigma * noise\n\n    if self._return_numpy:\n        if isinstance(val, torch.Tensor):\n            try:\n                return val.detach().cpu().numpy()\n            except RuntimeError as e:\n                if \"Numpy is not available\" in str(e):\n                    raise ImportError(\n                        \"NumPy conversion requested but NumPy is not properly installed or has compatibility issues. \"\n                        \"Please install/upgrade numpy: pip install -U numpy\"\n                    ) from e\n                raise\n        return np.asarray(val)\n    else:\n        # If val is somehow numpy/float, cast to tensor\n        if not isinstance(val, torch.Tensor):\n            val = torch.tensor(val, device=self._device)\n        return val\n</code></pre>"},{"location":"api/#torchfbm.rl.FBMActionNoise.reset","title":"<code>reset()</code>","text":"<p>Pre-generate a buffer of fGn samples.</p> <p>Called automatically during initialization and when the buffer is exhausted during sampling.</p> Source code in <code>torchfbm/rl.py</code> <pre><code>def reset(self):\n    \"\"\"Pre-generate a buffer of fGn samples.\n\n    Called automatically during initialization and when the\n    buffer is exhausted during sampling.\n    \"\"\"\n    if self._method == \"cholesky\":\n        gen_func = generate_cholesky\n    else:\n        gen_func = generate_davies_harte\n\n    fgn = gen_func(self._buffer_size, self._H, size=self._size, device=self._device)\n\n    if self._return_numpy:\n        try:\n            self._noise_buffer = (\n                fgn.detach().cpu().numpy()\n            )  # Convert to NumPy on CPU\n        except RuntimeError as e:\n            if \"Numpy is not available\" in str(e):\n                raise ImportError(\n                    \"NumPy conversion requested but NumPy is not properly installed or has compatibility issues. \"\n                    \"Please install/upgrade numpy: pip install -U numpy\"\n                ) from e\n            raise\n    else:\n        self._noise_buffer = fgn  # Keep as Tensor on Device\n\n    self._step = 0\n</code></pre>"},{"location":"api/#utilities","title":"Utilities","text":"<p>General utility functions for greater transparency and debugging.</p>"},{"location":"api/#torchfbm.utils","title":"<code>torchfbm.utils</code>","text":""},{"location":"api/#torchfbm.utils.get_cholesky_factor","title":"<code>get_cholesky_factor(n, H, device='cpu', dtype=torch.float32, jitter=1e-06)</code>","text":"<p>Returns the Lower Triangular Matrix L such that Sigma = L @ L.T. Useful for checking numerical stability or manual noise generation.</p> Source code in <code>torchfbm/utils.py</code> <pre><code>def get_cholesky_factor(\n    n: int, \n    H: float, \n    device=\"cpu\", \n    dtype=torch.float32,\n    jitter: float = 1e-6\n) -&gt; torch.Tensor:\n    \"\"\"\n    Returns the Lower Triangular Matrix L such that Sigma = L @ L.T.\n    Useful for checking numerical stability or manual noise generation.\n    \"\"\"\n    sigma = get_fgn_covariance_matrix(n, H, device, dtype)\n\n    # Add jitter for numerical stability (Conditioning)\n    eye = torch.eye(n, device=device, dtype=dtype)\n\n    try:\n        L = torch.linalg.cholesky(sigma + jitter * eye)\n    except RuntimeError:\n        # Fallback for higher jitter if matrix is nearly singular (common when H -&gt; 1.0)\n        L = torch.linalg.cholesky(sigma + jitter * 10 * eye)\n\n    return L\n</code></pre>"},{"location":"api/#torchfbm.utils.get_fgn_autocovariance","title":"<code>get_fgn_autocovariance(n, H, device='cpu', dtype=torch.float32)</code>","text":"<p>Computes the first row of the Toeplitz covariance matrix for fGN. gamma(k) = 0.5 * (|k+1|^2H - 2|k|^2H + |k-1|^2H)</p> Source code in <code>torchfbm/utils.py</code> <pre><code>def get_fgn_autocovariance(\n    n: int, \n    H: float, \n    device=\"cpu\", \n    dtype=torch.float32\n) -&gt; torch.Tensor:\n    \"\"\"\n    Computes the first row of the Toeplitz covariance matrix for fGN.\n    gamma(k) = 0.5 * (|k+1|^2H - 2|k|^2H + |k-1|^2H)\n    \"\"\"\n    k = torch.arange(n, device=device, dtype=dtype)\n    return 0.5 * (torch.abs(k + 1)**(2 * H) - 2 * torch.abs(k)**(2 * H) + torch.abs(k - 1)**(2 * H))\n</code></pre>"},{"location":"api/#torchfbm.utils.get_fgn_covariance_matrix","title":"<code>get_fgn_covariance_matrix(n, H, device='cpu', dtype=torch.float32)</code>","text":"<p>Constructs the full symmetric Toeplitz Covariance Matrix for fGN. Shape: (n, n)</p> Source code in <code>torchfbm/utils.py</code> <pre><code>def get_fgn_covariance_matrix(\n    n: int, \n    H: float, \n    device=\"cpu\", \n    dtype=torch.float32\n) -&gt; torch.Tensor:\n    \"\"\"\n    Constructs the full symmetric Toeplitz Covariance Matrix for fGN.\n    Shape: (n, n)\n    \"\"\"\n    # 1. Get the first row (Autocovariance)\n    gamma = get_fgn_autocovariance(n, H, device, dtype)\n\n    # 2. Use the Broadcasting Trick (No Loops)\n    # This creates the indices |i - j| efficiently\n    idx = torch.arange(n, device=device)\n    lhs = idx.unsqueeze(0)  # (1, n)\n    rhs = idx.unsqueeze(1)  # (n, 1)\n\n    # \"distance_matrix\" contains the lag k for every entry (i, j)\n    distance_matrix = torch.abs(lhs - rhs)\n\n    # 3. Map gamma values to the matrix\n    # PyTorch advanced indexing handles this instantly\n    return gamma[distance_matrix]\n</code></pre>"},{"location":"paper/","title":"Paper","text":"<p>title: 'torchfbm: A PyTorch Library for Differentiable Fractional Brownian Motion' tags:   - Python   - PyTorch   - Stochastic Processes   - Fractional Brownian Motion   - Deep Learning   - Anomalous Diffusion authors:   - name: Ivan Habib     orcid: 0009-0002-7812-0371</p> <pre><code>affiliation: 1\n</code></pre> <p>affiliations:  - name: Independent Researcher    index: 1 date: 22 January 2026 bibliography: paper.bib</p>"},{"location":"paper/#summary","title":"Summary","text":"<p><code>torchfbm</code> is a Python library built upon PyTorch that facilitates the simulation, analysis, and modeling of Fractional Brownian Motion (fBm) and Fractional Gaussian Noise (fGn). The library provides GPU-accelerated implementations of the Davies-Harte algorithm for exact simulation [@davies1987tests], alongside differentiable modules for integrating fractional noise into neural network architectures. <code>torchfbm</code> extends standard stochastic differential equation (SDE) solvers to the fractional regime, enabling the modeling of systems exhibiting anomalous diffusion and long-range dependence.</p>"},{"location":"paper/#statement-of-need","title":"Statement of Need","text":"<p>Fractional Brownian Motion is a fundamental stochastic process used to model phenomena with long-term memory in fields ranging from quantitative finance [@rogers1997arbitrage] to biophysics and hydrology [@mandelbrot1968fractional]. While Python packages such as <code>fbm</code> exist, they typically rely on NumPy, restricting computations to the CPU and preventing gradient propagation.</p> <p>As deep learning intersects with stochastic modeling\u2014specifically in Physics-Informed Neural Networks (PINNs) and Neural SDEs\u2014there is a critical need for a library that treats \\(H\\) (the Hurst exponent) not just as a static hyperparameter, but as a differentiable tensor. <code>torchfbm</code> addresses this by providing:</p> <ol> <li>Computational Efficiency: Leveraging PyTorch's FFT implementation to execute the Davies-Harte algorithm on GPUs, reducing generation time for large batches of long path lengths (\\(N &gt; 10^4\\)).</li> <li>Differentiability: Enabling end-to-end optimization of the Hurst exponent via spectral estimators and Detrended Fluctuation Analysis (DFA) [@peng1994mosaic].</li> <li>Modular Abstractions: Offering <code>nn.Module</code> wrappers for Noisy Nets [@fortunato2017noisy] and Neural fSDEs, facilitating the exploration of non-Markovian dynamics in Reinforcement Learning and Generative Modeling.</li> </ol>"},{"location":"paper/#mathematics-and-implementation","title":"Mathematics and Implementation","text":"<p>The core of <code>torchfbm</code> relies on the spectral embedding of the autocovariance sequence of fGn into a circulant matrix. For a process with Hurst exponent \\(H\\), the autocovariance \\(\\gamma(k)\\) is defined as:</p> \\[ \\gamma(k) = \\frac{1}{2} \\left(|k+1|^{2H} - 2|k|^{2H} + |k-1|^{2H} \\right) \\] <p>By computing the eigenvalues of the circulant matrix via Fast Fourier Transform (FFT), <code>torchfbm</code> generates exact fGn samples with \\(O(N \\log N)\\) complexity.</p> <p>The library also implements the Fractional Ornstein-Uhlenbeck process and Geometric fBm, serving as foundational building blocks for quantitative finance research.</p>"},{"location":"paper/#research-applications","title":"Research Applications","text":"<p><code>torchfbm</code> is designed to support research in:</p> <ul> <li>Reinforcement Learning: Using <code>FBMNoisyLinear</code> and <code>FBMActionNoise</code> to introduce temporally correlated exploration noise, which has been shown to improve convergence in sparse-reward environments.</li> <li>Generative Modeling: Training Neural SDEs to approximate rough volatility surfaces or time-series data with specific spectral scaling properties.</li> <li>Signal Processing: Utilizing GPU-accelerated DFA to estimate scaling exponents in high-frequency data streams.</li> </ul>"},{"location":"paper/#references","title":"References","text":""}]}